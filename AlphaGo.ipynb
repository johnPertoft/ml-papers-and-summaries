{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mastering the game of Go with deep neural networks and tree search\n",
    "This paper became kind of a milestone in AI since the game go is a lot harder than many other perfect information games like chess/checkers due to its very high breadth $\\approx 250$ (number of actions per state) and high depth $\\approx 150$ (number of total moves in a game).\n",
    "\n",
    "Perfect information games is basically that no information is ever hidden from any of the players. In this case there is an optimal value function $v^*(s)$ that determines the outcome of the game from all game states assuming perfect play by all players.\n",
    "\n",
    "The main idea behind creating AIs for these types of games is to find an approximate value function $v(s)$\n",
    "\n",
    "Is this true?: Randomness in actions from a state can still occurr as long as all information is visible in the next state. If so how can we have an optimal value function?\n",
    "\n",
    "previous solutions, tree search with state value functions to cutoff search depths, breadth can be cut down via sampling from a policy (distribution over actions in a state)\n",
    "\n",
    "## Background\n",
    "\n",
    "### Monte Carlo Tree Search (MCTS)\n",
    "In short MCTS is a way to estimate value of states in a game search tree. It does this by performing four steps\n",
    "* **Selection:** Starting from the current state and select actions until reaching the end of the game. Different strategies can be used for selecting actions.\n",
    "* **Expansion:** \n",
    "* **Simulation:** (also called playout/rollout)\n",
    "* **Backpropagation:**\n",
    "\n",
    "## AlphaGo\n",
    "There are multiple parts of AlphaGo.\n",
    "* Two policy networks, game states are input, and output is a distribution over moves from that state.\n",
    "* Value network, game states as input, chance of winning the whole game starting from the given state as output.\n",
    "\n",
    "\n",
    "### Data representation\n",
    "They do some different experiments with different number of features but the common representation is to keep the different features in separate feature planes where each plane is a 19x19 (Go board size) matrix. If a feature is count data ($\\in \\mathbb{Z})$ it is represented by several feature planes with a 1 at one position.\n",
    "\n",
    "\n",
    "### Training\n",
    "There are several steps to training this setup.\n",
    "\n",
    "1. Supervised learning of policy network $p_\\sigma$. \n",
    "    * Training data is expertmoves from corresponding gamestates.\n",
    "\n",
    "\n",
    "2. Reinforcement learning of policy network $p_\\rho$.\n",
    "    * Same architecture as $p_\\sigma$ and initialized to the same weights.\n",
    "    * $p_\\rho$ is improved by playing games against randomly selected previous versions of this policy network. Picking opponent randomly reduces risk of overfitting.\n",
    "    * The reward function gives zero for steps not ending the game, and gives +1 for winning and -1 for losing.\n",
    "    * Weights are updated at each timestep by following gradient in direction that maximizes expected outcome.\n",
    "\n",
    "\n",
    "3. Reinforcement learning of value network\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "TODO\n",
    "Policy network (SL and RL), SL initializes RL then selfplay\n",
    "Value network trained by getting position by self play with RL policy network\n",
    "Then combine these in MCTS, choose actions in rollout with policy network (?), use value network to evaluate states? \n",
    "\n",
    "Fast rollout thing??\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The first stage is to use supervised learning to train supervised policy network $p_\\sigma$. The training data is board states coupled with corresponding expert moves from those states. They also train with some additional features which gave slightly higher accuracy in predicting the expert moves. Higher accuracy here led to large improvements in playing strength but there is also a tradeoff since a more complex model with perhaps higher accuracy will take longer time to evaluate which could decrease playing strength.\n",
    "\n",
    "The second stage is the reinforcement learning stage. Here they have another policy network, $p_\\rho$, of the same architecture as $p_\\sigma$. The weights are first copied over to $p_\\rho$ from $p_\\sigma$.\n",
    "\n",
    "### Network details\n",
    "TODO: architecture, has it been published? conv + relus is stated in paper\n",
    "\n",
    "policy networks 13 layers, convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
