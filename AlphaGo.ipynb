{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mastering the game of Go with deep neural networks and tree search\n",
    "This paper became kind of a milestone in AI since the game go is a lot harder than many other perfect information games like chess/checkers due to its very high breadth $\\approx 250$ (number of actions per state) and high depth $\\approx 150$ (number of total moves in a game).\n",
    "\n",
    "Perfect information games is basically that no information is ever hidden from any of the players. In this case there is an optimal value function $v^*(s)$ that determines the outcome of the game from all game states assuming perfect play by all players.\n",
    "\n",
    "The main idea behind creating AIs for these types of games is to find an approximate value function $v(s)$\n",
    "\n",
    "Is this true?: Randomness in actions from a state can still occurr as long as all information is visible in the next state. If so how can we have an optimal value function?\n",
    "\n",
    "previous solutions, tree search with state value functions to cutoff search depths, breadth can be cut down via sampling from a policy (distribution over actions in a state)\n",
    "\n",
    "## Background\n",
    "\n",
    "### Monte Carlo Tree Search (MCTS)\n",
    "In short MCTS is a way to estimate value of states in a game search tree. It does this by performing four steps\n",
    "* **Selection:** Starting from the current state and select actions until reaching the end of the game. Different strategies can be used for selecting actions.\n",
    "* **Expansion:** \n",
    "* **Simulation:** (also called playout/rollout)\n",
    "* **Backpropagation:**\n",
    "\n",
    "## AlphaGo\n",
    "There are multiple parts of AlphaGo.\n",
    "* Two policy networks, game states are input, and output is a distribution over moves from that state.\n",
    "* Value network, game states as input, chance of winning the whole game starting from the given state as output.\n",
    "\n",
    "\n",
    "### Data representation\n",
    "They do some different experiments with different number of features but the common representation is to keep the different features in separate feature planes where each plane is a 19x19 (Go board size) matrix. If a feature is count data ($\\in \\mathbb{Z})$ it is represented by several feature planes with a 1 at one position.\n",
    "\n",
    "\n",
    "### Training\n",
    "There are several steps to training this setup.\n",
    "\n",
    "1. Supervised learning of policy network $p_\\sigma$. \n",
    "    * Training data is expertmoves from corresponding gamestates.\n",
    "\n",
    "2. Reinforcement learning of policy network $p_\\rho$.\n",
    "    * Same architecture as $p_\\sigma$ and initialized to the same weights.\n",
    "    * $p_\\rho$ is improved by playing games against randomly selected previous versions of this policy network. Picking opponent randomly reduces risk of overfitting.\n",
    "    * The reward function gives zero for steps not ending the game, and gives +1 for winning and -1 for losing.\n",
    "    * Weights are updated at each timestep by following gradient in direction that maximizes expected outcome.\n",
    "\n",
    "3. Reinforcement learning of value network to approximate optimal value function.\n",
    "    * Similar architecture to policy networks.\n",
    "    * Problems with overfitting if training this network on existing data (KGS dataset).\n",
    "    * Instead they generate data (state, outcome pairs) from self play, only taking one data point per game.\n",
    "    * Evaluating this network approached the accuracy of Monte Carlo rollouts using the RL policy network $p_\\rho$ but using 15000 times less computation.\n",
    "\n",
    "\n",
    "### Playing\n",
    "When AlphaGo then plays the game these networks are combined with MCTS that selects action by look ahead search.\n",
    "\n",
    "* Edges of search tree stores\n",
    "    * a visit count $N(s, a) = \\sum^n_{i=1} 1(s, a, i)$\n",
    "    * action value $Q(s, a) = \\frac{1}{N(s, a)}\\sum^n_{i=1} 1(s, a, i) V(s_L^i)$\n",
    "    * a prior probability $p(s, a)$\n",
    "\n",
    "In simulation, games are played to the end from the root state and actions are chosen by $a_t = argmax (Q(s_t, a) + u(s_t, a))$\n",
    "\n",
    "\n",
    "Fast rollout thing??\n",
    "\n",
    "\n",
    "### Network details\n",
    "TODO: architecture, has it been published? conv + relus is stated in paper\n",
    "\n",
    "policy networks 13 layers, convolutions\n",
    "\n",
    "### TODO\n",
    "the engineering part of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
