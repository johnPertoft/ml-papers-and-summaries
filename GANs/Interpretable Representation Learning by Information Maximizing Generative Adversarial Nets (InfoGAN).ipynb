{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets\n",
    "\n",
    "This paper introduces an extension to the GAN framework to learn disentangled representations completely unsupervised.\n",
    "\n",
    "Disentangled representation means that different factors/dimensions of the representation will be correspond to different types of semantic change in the output. I.e. walking along one dimension might change only the thickness of generated digits.\n",
    "\n",
    "## Theory: Mutual information for inducing latent codes\n",
    "* Normal GANs just use a simple factored continuous noise vector $z$ as input.\n",
    "    * No restrictions on how this input is used by the generator.\n",
    "    * Thus possible (and likely?) that $z$ will be used in a highly entangled way.\n",
    "\n",
    "\n",
    "* Instead let the input consist of two parts:\n",
    "    * $z$ same as in normal GANs.\n",
    "    * $c = [ c_1, c_2, \\dotsc, c_L ]$ used to capture the salient structured semantic features of the data.\n",
    "        * Assume a factored distribution $p(c_1, \\dotsc, c_L) = \\prod^L_{i=1} p(c_i)$\n",
    "\n",
    "\n",
    "* To force G to parametrize a distribution that actually uses $c$ they add *information-theoretic* regularization.\n",
    "    * Want high mutual information between $c$ and generator distribution $G(z, c)$\n",
    "    * Mutual information: $I(A; B) = H(A) - H(A\\ \\lvert\\ B) = H(B) - H(B\\ \\lvert\\ A)$, $H$ is entropy.\n",
    "    * Intuition: mutual information measures how much information is gained by conditioning on another variable.\n",
    "        * Independence means mutual information is zero.\n",
    "        * If $c$ \"changes\" the generative distribution the mutual information is higher, which we want.\n",
    "    * For $x \\sim p_G(x) = G(z, c)$, we want $p(c\\ \\lvert\\ x)$ to have small entropy, i.e. the generative process should not lose the information stored in $c$.\n",
    "    * We add $-\\lambda I(c;G(z, c))$ to the GAN objective.\n",
    "    \n",
    "    \n",
    "* Can't maximize $I(c;G(z, c))$ directly since we need the posterior $p(c\\ |\\ x)$.\n",
    "    * Instead, use technique called *Variational Information Maximization*.\n",
    "        * TODO: lower bound, same idea as variational inference?\n",
    "        * Q distribution\n",
    "    * $I(c;G(z, c)) = H(c) - H(c\\ \\lvert\\ G(z, c)) \\geq \\mathbb{E}_{x \\sim G(z, c)} \\left[ \\mathbb{E}_{c' \\sim P(c\\lvert x)} \\left[ log\\ Q(c'\\ \\lvert\\ x) \\right] \\right] + H(c) = \\mathcal{L}_I$\n",
    "    * With this formulation, still need to sample from posterior in inner expectation so they use an identity to define the loss in the following way.\n",
    "    * $\\mathcal{L}_I = \\mathbb{E}_{c \\sim p(c), x \\sim G(z, c)} \\left[ log\\ Q(c\\ \\lvert\\ x) \\right] + H(c)$\n",
    "    * $H(c)$ can be optimized over as well, but in this work they keep $p(c)$ fixed. Thus consider $H(c)$ constant.\n",
    "    * $\\mathcal{L}_I$ can be approximated with Monte Carlo simulation + reparametrization trick.\n",
    "\n",
    "## Implementation\n",
    "* Q parametrized as a neural network that shares parameters with D.\n",
    "    * One extra fully connected layer to out put parameters for $Q(c\\ \\lvert\\ x)$\n",
    "    * For categorical $c_i$ use softmax for $Q(c_i\\ \\lvert\\ x)$\n",
    "    * For continuous $c_j$, can use other things but factored gaussian usually enough.\n",
    "* $\\lambda$ parameter easy to tune.\n",
    "* They use DCGAN architecture.\n",
    "\n",
    "## Experiments\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
