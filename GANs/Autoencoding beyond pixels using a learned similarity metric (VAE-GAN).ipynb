{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoding beyond pixels using a learned similarity metric\n",
    "* In short this paper combines a variational autoencoder (VAE) with a generative adversarial network (GAN) to form the VAE-GAN model.\n",
    "\n",
    "* The idea is to use the feature representations that the discriminator of a GAN learns in the reconstruction objective of the VAE. Normal VAE is too concerned with pixel-wise errors but doing it this way should help with instead minimizing feature-wise errors to better capture the data distribution.\n",
    "\n",
    "* Pixel-wise errors are not optimal for image data because they do not really model human visual perception. Small image translation that a human doesn't care about will give large pixel-wise errors.\n",
    "\n",
    "* They also show some disentanglement of latent factors.\n",
    "\n",
    "## VAE recap\n",
    "$z \\sim Enc(x) = q(z\\ \\lvert\\ x), \\quad \\hat{x} \\sim Dec(z) = p(x\\ \\lvert\\ z)$\n",
    "\n",
    "VAE loss consists of two parts\n",
    "* Regularization on latent space to be close to prior over $z$ (often $p(z) = \\mathcal{N}(0, 1)$) via KL divergence $\\mathcal{L}_{prior} = KL(q(z\\ \\lvert\\ x) \\lVert p(z))$\n",
    "* Reconstruction loss which is the negative expected log likelihood (pixel-wise).\n",
    "\n",
    "## GAN recap\n",
    "* Two networks, generator and discriminator\n",
    "    * Generator generates data samples from noise $z$.\n",
    "    * Discriminator outputs probability that a sample is real (vs fake).\n",
    "    * Binary cross entropy $\\mathcal{L}_{GAN} = log(D(x)) + log(1 - D(G(z))$, minimize for training G, maximize for training D.\n",
    "\n",
    "## VAE-GAN\n",
    "\n",
    "### Architecture\n",
    "<img src=\"figs/vaegan/vaegan.png\" width=\"30%\" height=\"30%\">\n",
    "\n",
    "### Losses\n",
    "* The discriminator of GANs learns a good feature representation of data which we want to use for a loss.\n",
    "* They replace the reconstruction loss part of the VAE loss with a reconstruction loss expressed in discriminator.\n",
    "    * Pick a layer $D_l$ of discriminator.\n",
    "    * Choose gaussian as distribution over output at this layer, $p(D_l(x)\\ \\lvert\\ z) = \\mathcal{N}(D_l(x)\\ \\lvert\\ D_l(\\hat{x}), \\mathbf{I})$\n",
    "    * The new reconstruction error is $\\mathcal{L}_{llike}^{D_l} = -\\mathbb{E}_{q(z \\lvert x)} \\left[ log\\ p(D_l(x)\\ \\lvert\\ z) \\right]$\n",
    "    * So we minimize squared difference between $x$ and $\\hat{x}$\n",
    "    * They call this learned similarity.\n",
    "* Also use augmented GAN loss described below.\n",
    "    \n",
    "### Training\n",
    "* Discriminating based on samples from $p(z)$ and $q(z \\lvert x) = Enc(x)$\n",
    "    * $\\mathcal{L}_{GAN} = log(D(x)) + log(1 - D(G(z)) + log(1 - D(G(Enc(x))$\n",
    "* Limiting error signals to relevant network by using parts of whole loss for encoder, decoder/generator, and discriminator.\n",
    "    * Encoder is trained by minimizing $\\mathcal{L}_{prior} + \\mathcal{L}_{llike}^{D_l}$ w.r.t $\\theta_{encoder}$.\n",
    "    * Decoder/Generator is trained by minimizing $\\gamma \\mathcal{L}_{llike}^{D_l} + \\mathcal{L}_{GAN}$ w.r.t $\\theta_{decoder}$ (so in effect minimize $\\mathcal{L}_{GAN}$ becomes minimize $log(1 - D(G(z)) + log(1 - D(G(Enc(x)))\\ $).\n",
    "    * Discriminator is trained by maximizing $\\mathcal{L}_{GAN}$\n",
    "* $\\gamma$ weight interpreted as weight between style and content.\n",
    "\n",
    "<img src=\"figs/vaegan/vaegan-pseudocode.png\" width=\"36%\" height=\"36%\">\n",
    "\n",
    "### Experiments\n",
    "* 64x64 images.\n",
    "* They state as many others that log likelihood measures do not correlate with visual fidelity.\n",
    "* Fractional striding (\"deconvolutions\") with stride 2\n",
    "* RMSProp(3e-4)\n",
    "* Batch size 64\n",
    "\n",
    "#### Visual attribute vectors, CelebA\n",
    "* Images in dataset are annotated with binary attributes (glasses, pale skin, etc).\n",
    "* Aligned dataset\n",
    "* They inspect the latent space to find directions corresponding to semantic features.\n",
    "    * Use encoder to get latent representation.\n",
    "    * For each binary attribute:\n",
    "        * Compute mean representation vectors of all images with and without the attribute.\n",
    "        * Visual attribute vector as difference of the two mean vectors.\n",
    "    * Not perfect, but captures most of the attributes.\n",
    "\n",
    "#### Attribute similarity, Labeled faces in the wild\n",
    "* Align faces as preprocessing\n",
    "* Concatenate attribute vector to inputs of encoder, decoder, and discriminator. For encoder and discriminator, concat to first fully connected layer.\n",
    "* Have a regression network (similar architecture as encoder) to predict attributes.\n",
    "* Not a lot of details \n",
    "\n",
    "#### Supervised tasks, CIFAR-10, STL-10\n",
    "* Semisupervised setup\n",
    "* Pretrain unsupervised\n",
    "* Not sure how they use the label\n",
    "* Not competetive results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
