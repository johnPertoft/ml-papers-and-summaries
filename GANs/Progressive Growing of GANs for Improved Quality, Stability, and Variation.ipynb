{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progressive Growing of GANs for Improved Quality, Stability, and Variation\n",
    "This paper\n",
    "* Proposes a new method for training GANS that both yields training speed up as well as more stable training. It is also orthogonal to other stabilization methods.\n",
    "* Proposes a new method to get increased variation in the learned distribution.\n",
    "* Suggests a new metric for evaluation of GANs.\n",
    "* They evaluate both on standard datasets (SoTA on CIFAR10) and on a new high resolution dataset with great results.\n",
    "\n",
    "## Introduction\n",
    "* They point out the common GAN training errors\n",
    "    * Gradients from \"distance\" metrics between training data distribution and generated distribution\n",
    "    can point in more or less random directions if little to no overlap in these distributions.\n",
    "* Other papers have suggested ways to improve this\n",
    "    * By using other distance metrics or in other ways put constraints to not let this happen (WGAN/WGAN-GP, BGAN, LSGAN, EBGAN, etc).\n",
    "* These problems are more common for higher resolutions because more chances for errors leading to discriminator having it too easy.\n",
    "    \n",
    "## Progressive Growing of GANs\n",
    "In short\n",
    "* Both generator and discriminator are initially defined to produce/handle very low resolution.\n",
    "* Then layers that produce/handle increased resolution are added to respective network progressively during training. Growing happens at the same time for both.\n",
    "* The idea is that this allows for first learning about large scale structure and then later having to focus on small details.\n",
    "    * A form of curriculum learning\n",
    "    * Intuitively it's simpler to turn a 512x512 image to a 1024x1024 than it is to take the initial latent vector to the full resolution image.\n",
    "    * Initially fewer modes.\n",
    "* The generator and discriminator have the same (but flipped) architecture.\n",
    "* New layers are \"faded in\" by\n",
    "    * First scaling the second to last (prev. last layer) to new resolution via nearest neighbor filtering and average pooling respectively.\n",
    "    * Then adding a bypass connection (residual block) around the new layer to a weighted sum of the old (up-/downscaled) layer and the new layer.\n",
    "    * $(1 - \\alpha) * old + \\alpha * new$\n",
    "    * $\\alpha$ is increased linearly from 0 to 1 during some amount of batches.\n",
    "* At each layer/resolution they have a 1x1 convolution layer to convert layer output to and from RGB to allow for more than 3 filters.\n",
    "* For training discriminator they downscale images to fit the current resolution.\n",
    "    * During fade in of a new layer, they employ the same technique described above for the real images too.\n",
    "    \n",
    "<img src=\"../figs/prog_gan/proggan.png\">\n",
    "\n",
    "## Minibatch Standard Deviation\n",
    "To increase variation in the generated distribution they use a variant of *minibatch discrimination*.\n",
    "* *minibatch discrimination*: add the following layer at the end of the discriminator\n",
    "    * Idea: Compute feature statistics at some layer across a minibatch to pass this information to discriminator.\n",
    "    * Take features of all samples and map these through through shared weights to produce a separate set of statistics such that each sample has information from all other samples in the minibatch.\n",
    "    * These are concatenated to the individual features.\n",
    "* *This variant*\n",
    "    * No learnable parameters or hyper parameters.\n",
    "    * Compute std of each feature for each spatial position across minibatch.\n",
    "    * These are averaged into a single scalar.\n",
    "    * Replicate this to form a new feature map this is concatenated on the samples in the minibatch.\n",
    "    * Best results if inserted at end of network.\n",
    "    \n",
    "## Normalization in Generator and Discriminator\n",
    "* The competition between generator and discriminator can sometimes cause escalation of signal magnitudes.\n",
    "* Usually combated with normalization techniques, like batch-normalization or layer normalization, in genarator (and sometimes discriminator).\n",
    "* They theorize that covariate shift (which these tecniques were originally intended to deal with) is not an issue for GANs.\n",
    "* Instead just focus on constraining signal magnitudes.\n",
    "\n",
    "## Evaluation Metrics\n",
    "\n",
    "### Multi-Scale Structural Similarity MS-SSIM\n",
    "* \n",
    "\n",
    "\n",
    "### Sliced Wasserstein Distance (SWD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
