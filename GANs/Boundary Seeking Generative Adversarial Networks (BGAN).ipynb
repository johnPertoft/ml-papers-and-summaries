{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Boundary Seeking GANs\n",
    "This paper presents another (slightly different) way  of training GANs with the aim of stabilizing training.\n",
    "\n",
    "In short the objective can be described as training G to produce samples that lie on the decision boundary of D.\n",
    "\n",
    "They also talk about another approach which can be used to train GANs with discrete output. It doesn't really seem connected to the boundary seeking part though. TODO\n",
    "\n",
    "It is also very easy to implement, at least in the continuous output case.\n",
    "\n",
    "## Background\n",
    "The GAN framework has many advantages but has problems with generating discrete output. This is because the composition of G and D must be differentiable and if using a step function to get discrete output at the end of G, it is not differentiable.\n",
    "\n",
    "Another way of thinking about the problem of discrete output is that for continuous output G might output $0.12345$ somewhere and the gradient from G's objective then tells it to generate something slightly smaller or larger then it can output $0.125$ next time maybe. But for discrete output, maybe G outputs $a \\in \\{a, b, c, d, e, f\\}$, but in this case G can't take a small step in some direction to better fool D since there is no notion of values between the values in $\\{a, b, c, d, e, f\\}$. \n",
    "\n",
    "---\n",
    "\n",
    "**Side note:** I have seen many implementations where data is like binary (3D-GAN paper, binary voxels) where they simply put a sigmoid at the end which means that it's differentiable obviously. What I'm not really sure about is why D can't just learn that generated samples have values like 0.9995 and real samples are either 0 or 1 but nothing in between and then discriminate perfectly based on that. It still worked in that paper and in things I've tried as well so what is going on there? Can we put some noise on all inputs to D to get around this?\n",
    "\n",
    "---\n",
    "\n",
    "In GANs we have the following objective\n",
    "$$\\min_G \\max_D \\mathbb{E}_{p_{data}(x)} \\left[ log\\ D(x) \\right] + \\mathbb{E}_{p_z(z)} \\left[ log (1 - D(G(z))) \\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
