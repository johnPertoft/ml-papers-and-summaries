{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Case for Learned Index Structures\n",
    "\n",
    "## Introduction\n",
    "* Key insight: Indexes are models.\n",
    "* The current (non ml) ones are general and not exploiting the patterns in the actual data being indexed.\n",
    "    * Real world data follows some (complex) distribution. We can learn this distribution to make better predictions.\n",
    "* Introduces the idea of using ML in systems where it has not been traditionally applied.\n",
    "    * Jeff Dean talk at NIPS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Types of Indexes\n",
    "The paper discusses a few different types of indexes which they try to improve by using neural nets to learn the indexes.\n",
    "* Range Indexes\n",
    "* Point Indexes\n",
    "* Existence Indexes\n",
    "\n",
    "### Main difficulties\n",
    "* How to provide the same guarantees as the non ml models give?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Range Index\n",
    "* Used when we want a mapping from a lookup key to a position in a sorted array of data records with the guarantee that the key of the record at the position is equal or higher than the lookup key.\n",
    "* E.g. B-tree index\n",
    "    * Map key to a page of records\n",
    "    * B-trees can be seen as regression trees, predict position with min- and max-error.\n",
    "    * Error is in how many positions off the prediction was.\n",
    "    * For B-trees, position predictions have min error of 0 and max error of page size.\n",
    "\n",
    "<img src=\"figs/learned_indexes/btree.png\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Learned Range Index\n",
    "* Want learned model $f$ to given key, predict position with some min- and max-error.\n",
    "* When training the learned model, keep track of the min- ($err_{min}$) and max-error ($err_{max}$) of it.\n",
    "* At prediction $f(key)$, search positions $[f(key) - err_{min}, f(key) + err_{max}]$ which has the same guarantees as a B-tree.\n",
    "* This model can be any type of regression model, like linear regression or a neural net.\n",
    "* An important insight is that range index models approximates the CDF of keys.\n",
    "    * $cdf(key) = p(x \\leq key)$, $cdf(key) \\in [0, 1]$\n",
    "    * Position prediction is then $p = cdf(key) * N$ where $N$ is number of keys.\n",
    "\n",
    "<img src=\"figs/learned_indexes/range_learned_idx.png\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### First approach: Naive Model\n",
    "* Small MLP trained to predict position given timestamps in web server log data.\n",
    "* Good approximation of general shape of CDF but problems at the individual sample level. The \"last mile\" accuracy is an issue.\n",
    "* Optimizing for average error isn't optimal in this case where we need small min- and max-error to find a record fast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Second approach: Recursive Model\n",
    "* Using just one model, it's easy to reduce large errors in prediction but hard for the very small ones. \"Last mile\" accuracy.\n",
    "* Instead use a hierarchy of models (ordered by stages) with different responsibilities. Mixture of experts.\n",
    "* Each sub model takes key as input and predicts position which is used to pick the next sub model in the next stage $l$ by $\\lfloor M_l f_{l-1}(x)/N \\rfloor$.\n",
    "    * Basically divide the keyspace in $M_l$ parts and pick the next sub model responsible for the space the previous prediction fell in.\n",
    "    * The picked model then also takes the key as input and so on.\n",
    "* Last stage makes actual prediction.\n",
    "* Each sub model is then responsible for different subspaces of the key space at increasing resolution.\n",
    "    * The ones in the bottom is good at predicting within a small subspace of keys. Solves the \"last mile\" problem.\n",
    "* Training is done by minimizing the squared error for the used sub models for a given prediction.\n",
    "    * Trained stage by stage.\n",
    "    * No gradient propagation between stages. Each sub model independently trained on (key, position) pairs in the relevant space.\n",
    "* Each sub model can be different and even be a B-tree.\n",
    "    * E.g. First some neural networks and then B-trees in the bottom stage.\n",
    "\n",
    "<img src=\"figs/learned_indexes/rmi.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Results for Map data set (keys are longitude coordinates).\n",
    "    * They also tried different search strategies within each position interval returned by the learned models.\n",
    "    * The models tried here are two stage with an initial mlp with 0-2 hidden layers and then a lot of linear regressors.\n",
    "    \n",
    "\n",
    "<img src=\"figs/learned_indexes/map_data_rmi.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Note on string keys\n",
    "* They try using fixed length strings (padded with 0 or truncated) by just converting to ascii values of individual characters.\n",
    "* This key is then a vector.\n",
    "* Want to model interactions between characters. Important for modeling CDF.\n",
    "* Open for future research. Could use RNNs and/or CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Note on insertions\n",
    "* For B-trees, can set min- and max-fillfactor of pages to leave some space for insertions.\n",
    "* Can possibly do the same but smarter by considering the CDF to know where to leave more space.\n",
    "* Also a trade off with the \"last mile\" accuracy. This is basically overfitting so with very high accuracy it would be harder to leave space for new keys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Point Index\n",
    "* Used when we want to look up a record in an unordered array.\n",
    "* E.g. a hash map.\n",
    "    * Hash function to map key to random positions in this array.\n",
    "    * Collisions are a problem that maybe can be alleviated by learning a model that more uniquely maps keys to positions.\n",
    "* Hard to beat hash map speedwise (with enough space allotted) but can save space.\n",
    "* Knowing something about the CDF is helpful in this case.\n",
    "    * Can define hash function $h(k) = f(k) * M$ where $M$ is size of hash map.\n",
    "    * Can use the same recursive model architecture from before to learn the CDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"figs/learned_indexes/point_index_results.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Existence Index\n",
    "* Used when we just want to know if a key is in a set.\n",
    "* E.g. bloom filter.\n",
    "    * M sized bit array.\n",
    "    * k hash functions.\n",
    "    * Insertion of $e$, compute $hash_k(e) \\forall_k$ and set those indexes to 1.\n",
    "    * To test for inclusion of $e$, compute $hash_k(e) \\forall_k$. If any bit is zero, it's not in the set.\n",
    "    * Guaranteed no false negatives. Might have false positives.\n",
    "    * Can lower false positive rate by sacrificing space\n",
    "* Learned Existence Index as binary classifier.\n",
    "    * Train a neural net for this binary classification task\n",
    "    * Tune the false positive rate by setting a probability threshold $\\gamma$ for predicting that a key is in the set.\n",
    "    * For all the false negatives from this model, use a normal bloom filter. Then we get the same guarantees but likely using less space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Discussion\n",
    "* When doesn't it work well?\n",
    "* Consequences of using this type of models. Harder to debug?\n",
    "* Focus here was on read only? They sketched some ideas for how to do this for write heavy applications as well.\n",
    "    * They say (re)indexing of B-tree is analogous to training of the RMI model. Similar time for a fit?\n",
    "* Key takeaways?\n",
    "    * \"we believe that the idea of replacing core components of a data management system through learned models...\""
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
