{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transformers overview\n",
    "\n",
    "- What do they do?\n",
    "- How do they work?\n",
    "- How can they be applied?\n",
    "\n",
    "Illustration credits: http://jalammar.github.io/illustrated-transformer/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "<img src=\"figs/transformers-overview/attention-is-all-you-need-arxiv.png\" width=\"50%\"></img>\n",
    "\n",
    "- At the time, people would use RNNs or CNNs together with attention mechanisms for complex sequence modeling.\n",
    "- The attention mechanism allowed for shortcuts in dependencies between token positions far apart.\n",
    "- With the Transformer architecture, the recurrence was dropped in favor of using only attention.\n",
    "- Without the recurrence, the Transformer architecture allows for a higher degree of parallelization since it's not limited by the sequential computation of RNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overview\n",
    "- The attention is all you need paper discusses the transformer architecture / layer in a sequence to sequence context (neural machine translation) with an encoder/decoder setup.\n",
    "- Will go through the parts of this setup in this paper first.\n",
    "- But can also use just the decoder (GPT-{1, 2, 3}, et al) or just the encoder (BERT, et al).\n",
    "- The Transformer building block is pretty versatile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overview\n",
    "<img src=\"figs/transformers-overview/transformers-1.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overview\n",
    "<img src=\"figs/transformers-overview/transformers-2.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overview\n",
    "<img src=\"figs/transformers-overview/transformers-3.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Encoder\n",
    "<img src=\"figs/transformers-overview/transformers-4.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Encoder\n",
    "<img src=\"figs/transformers-overview/transformers-5.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Self-attention\n",
    "- When processing an embedding at a certain position, self-attention allows the model to consider the other positions in the input embedding sequence in order to produce a better output embedding at that position.\n",
    "- The example below shows the self-attention scores on the other words when processing the word \"it\".\n",
    "- Here it seems like the model is paying more attention to the words \"the animal\" which is what the \"it\" word refers to in this context.\n",
    "\n",
    "<img src=\"figs/transformers-overview/transformers-6.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Self-attention\n",
    "- At every position, the embedding $x_i$ is taken through three transformations $\\mathbf{W}_Q$, $\\mathbf{W}_K$, and $\\mathbf{W}_V$ to produce the query ($q_i$), key ($k_i$), and value ($v_i$) vectors respectively.\n",
    "\n",
    "<img src=\"figs/transformers-overview/transformers-7.png\" width=\"50%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Self-attention\n",
    "- At a certain position, the self-attention scores of all the positions in the same sequence are computed by taking the dot product of the **query** vector $q_i$ with all **key** vectors $k_j$, dividing by $\\sqrt{d_k}$, and then passing through a softmax.\n",
    "- The self-attention scores are then used as the weights in a weighted sum of the **value** vectors $v_j$.\n",
    "- This weighted sum is the output of the self-attention layer.\n",
    "- In matrix form, this looks like \n",
    "$$\\mathbf{X} \\times \\mathbf{W}_Q = \\mathbf{Q}$$\n",
    "$$\\mathbf{X} \\times \\mathbf{W}_K = \\mathbf{K}$$\n",
    "$$\\mathbf{X} \\times \\mathbf{W}_V = \\mathbf{V}$$\n",
    "$$attention(Q, K, V) = softmax(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_k}})\\mathbf{V}$$\n",
    "- $d_k$ is the size of the query and key vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multihead attention\n",
    "- The attention is actually implemented as multihead attention which is essentially just the attention as already described but done multiple times with $h$ separate sets of linear transformations on the input queries, keys, values.\n",
    "- The $h$ different outputs are then concatenated and projected down to the final output.\n",
    "\n",
    "<img src=\"figs/transformers-overview/transformers-fig2.png\" width=\"75%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decoder\n",
    "- The decoder layers are the same as the encoder layers but with two differences:\n",
    "- First, the addition of another attention layer that instead attends over the encoder outputs.\n",
    "    - In this case the query vectors are from the decoding sequence but the key and value vectors are from the encoding output sequence.\n",
    "- Second, in the self-attention of the decoding layers, attending to future positions is not allowed. I.e. at position $i$, the model is only allowed to attend to positions $j <= i$\n",
    "    - This is implemented through \"masking\" the future positions by ensuring that they have low softmax scores by setting these positions to $-inf$\n",
    "\n",
    "<img src=\"figs/transformers-overview/transformers-8.png\" width=\"50%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Positional embeddings\n",
    "- The transformer architecture doesn't have any notion of order of the embeddings since all the attention scores are computed in parallel.\n",
    "- In this work, the authors inject this information by adding positional embeddings to the input embeddings of both the encoder and the decoder.\n",
    "- There are different choices for positional embeddings, but here they use fixed embeddings computed from many sinusoids at different frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summarized architecture\n",
    "<img src=\"figs/transformers-overview/transformers-fig1.png\" width=\"50%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training (neural machine translation)\n",
    "- The decoder has a linear+softmax output layer that predicts the next token from the output embedding at a certain position.\n",
    "- The encoder and decoder are trained end-to-end by maximizing the likelihood of the ground truth word in the output distribution at every position.\n",
    "- The ground truth words are passed in to the decoder but shifted one step to the right. This is called teacher forcing.\n",
    "- Remember that the decoder has the masking to ensure that it can not look into the future steps when computing its output anywhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding\n",
    "- When using the model to translate an input text, the input text is passed through the encoder in the same way.\n",
    "- But the decoder is now decoding in an autoregressive manner, i.e. the predicted output at a step is used as the input to the following step.\n",
    "- It's possible to cache intermediate calculations when doing this instead of recomputing everything with the \"new\"  input sequence.\n",
    "- There are different strategies for decoding/generating a sequence with the decoder.\n",
    "- The simplest would be **greedy decoding** which is just to take the top predicted word at every step.\n",
    "- Another very simple would be **random sampling** based on the output distribution.\n",
    "- Another method would be **beam-search** which considers multiple \"paths\" or hypotheses when picking the next token.\n",
    "- Lately, people have been using other strategies like **top-k sampling** and **nucleus sampling** but we can talk about those some other time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Transformers since 2017\n",
    "Some transformer based models that have come out since when the original paper was released. It's by no means an extensive list and I've probably missed a bunch of interesting papers. I think we should revisit this section later.\n",
    "\n",
    "<img src=\"figs/transformers-overview/transformer-models.png\" width=\"30%\"></img>\n",
    "<img src=\"figs/transformers-overview/transformer-models2.png\" width=\"30%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GPT\n",
    "- The GPT models (1, 2, 3) are large generative language models.\n",
    "- Architecturewise, it's \"just\" an upscaled version of the decoder part of the previously described architecture trained on massive amounts of text.\n",
    "- Trained like any other language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## BERT\n",
    "- BERT is a language understanding model.\n",
    "- BERT is first pretrained on massive amounts of unlabeled text using a masked language modeling objective.\n",
    "- BERT can then be finetuned on many different downstream tasks like sentence classification by attaching another head and finetuning it.\n",
    "- The masked language modeling as a pretraining objective has since been used in a ton of other models in different variants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## TabNet\n",
    "- Transformer for tabular data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## VisionTransformer\n",
    "- Transformer encoder applied to image classification, i.e. replacing convolutions.\n",
    "- They split the image into patches, linearly project flattened patches into embeddings and feed them through the encoder.\n",
    "- Trained with a classification head in image classification problems and achieve state of the art results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Visual BERT models\n",
    "- There's couple of models exploring adding a visual modality to BERT.\n",
    "- VisualBert, VLBert, ViLBERT are some of them.\n",
    "- They all use similar ideas with taking patches of images as visual words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CLIP and DALL-E\n",
    "- CLIP predicts the similarity of an image and a text description.\n",
    "- DALL-E generates an image given a text prompt, as seen in the (possibly cherry-picked) example below.\n",
    "\n",
    "<img src=\"figs/transformers-overview/dall-e.png\" width=\"50%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Research directions\n",
    "TODO\n",
    "\n",
    "<img src=\"figs/transformers-overview/long-range-transformers-taxonomy.png\" width=\"50%\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
