{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On the variance of adaptive learning rate and beyond (RAdam)\n",
    "\n",
    "## Motivation\n",
    "- Adaptive learning rate optimizers have benefits over normal SGD, like faster convergence, less sensitive to hyper params.\n",
    "- Lately, the learning rate warmup heuristic has seen a lot of use with adaptive learning rate optimizers because of improved training stabilization, faster convergence, improved generalization.\n",
    "- In many recent NLP models, the warmup heuristic has been essential for it to train successfully.\n",
    "- The reasons for this have not been investigated.\n",
    "- The warmup heuristic comes with extra hyper parameters that must be tuned.\n",
    "- This paper theoretically and empirically looks at the reasons why adaptive learning rate optimizers sometimes fail and suggests a new optimizer algorithm to alleviate these problems.\n",
    "\n",
    "## Background\n",
    "\n",
    "### Adaptive learning rate optimizers\n",
    "- Can be described in a general way like this.\n",
    "\n",
    "<img src=\"figs/radam/generic-adaptive-learning-rate-algo.png\" width=\"60%\"/>\n",
    "\n",
    "- In the Adam case we have\n",
    "\n",
    "<img src=\"figs/radam/adam-specific-case.png\" width=\"60%\"/>\n",
    "\n",
    "### Warmup heuristic\n",
    "- The learning rate $\\alpha_t$ is set to a very small value initially. \n",
    "- Then (often linearly) increased to some target learning rate. \n",
    "- The warmup period is usually over one epoch or similar.\n",
    "- After warmup other learning rate scheduled might be applied like some form of decay.\n",
    "\n",
    "## Analysis\n",
    "- The figure shows that the distribution of (absolute) gradient values are quickly distorted.\n",
    "- This is because the moving averages haven't seen enough samples so their estimates are bad with too high variance which in turn will affect the adaptive learning rate.\n",
    "- The warmup heuristic solves this by dampening the step sizes early on when too few samples have been seen.\n",
    "\n",
    "<img src=\"figs/radam/radam-gradient-histogram.png\" width=\"40%\"/>\n",
    "\n",
    "- They verify this by an experiment in which they use normal Adam but initially only updates the adaptive learning rate of it for 2000 iterations. The model parameters and momentum variables are kept frozen. This variant avoids the problems.\n",
    "- They verify this by another experiment in which they explicitly decrease the variance by increasing the $\\epsilon$ to a non neglible value. This also avoids the problem but has worse performance. $\\epsilon$ is usually set higher when training imagenet for example which I suppose is maybe related to this?\n",
    "\n",
    "<img src=\"figs/radam/adam-verification-experiments.png\" width=\"60%\"/>\n",
    "\n",
    "- They then verify this analytically as well.\n",
    "\n",
    "## Rectified adaptive learning rate\n",
    "- Add the rectifier term (TODO: Intution?)\n",
    "- Turn on or off the adaptiveness depending on the divergence of the variance (TODO: Intution?)\n",
    "\n",
    "<img src=\"figs/radam/radam-algo.png\" width=\"50%\"/>\n",
    "\n",
    "## Comparison with learning rate warmup\n",
    "TODO\n",
    "\n",
    "## Experiments\n",
    "TODO\n",
    "<img src=\"figs/radam/radam-lr-robustness.png\" width=\"80%\"/>\n",
    "\n",
    "## Takeaways\n",
    "- Removes the need for learning warmup phase, less hyper parameters.\n",
    "- Less sensitive to choice of learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
