{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ICML2018 Notes\n",
    "Some notes taken during ICML 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Temporal Point Processes\n",
    "* Models for discrete events in continuous time, irregular events?\n",
    "* E.g. financial data, social media activity, users articles data\n",
    "* Intensity function $\\lambda(t) = \\frac{P(\\text{event happening between t and t+dt)}}{dt}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Toward Theoretical Understanding of Deep Learning\n",
    "* Optimization\n",
    "    * We don't really know that much yet, many studies are done on small networks or networks without nonlinearities.\n",
    "* Overparametrization\n",
    "    * e.g. why doesn't a big network with 20M params overfit on cifar10 with 50K samples? We don't really know why.\n",
    "    * Folklore(?) experiment\n",
    "        * Generate labeled data by feeding random data into a depth 2 net.\n",
    "        * Then try to train a new network to learn this. Much easier with a big network.\n",
    "    * Longtime belief: SGD + regularization eliminated \"excess capacity\"\n",
    "    * Noise stability of trained networks, i.e. if injecting noise at some layer, difference in output further down should be small. We don't really know why though.\n",
    "        * Generalization\n",
    "        * This could be used to prove that the network is compressible.\n",
    "* Role of depth\n",
    "    * Expressiveness but also about making optimization easier which is a bit counter intuitive.\n",
    "* Mentioned during questions after: *The lottery ticket hypothesis* paper - with a big network we get more \"chances\" at finding a solution that can then be pruned leading to better performance than starting with the pruned architecture.\n",
    "    1. Train big network, prune it.\n",
    "    2. Take pruned network, reset weights to their initial value (from big). \"The winning ticket.\"\n",
    "    3. Train this network. Better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Variational Bayes and Beyond: Bayesian Inference for Big Data\n",
    "* Good introduction about variational inference in general and how it fits in among different techniques in approximate bayesian inference. The following describes subset of approximate inference leading down to how to optimize variational parameters for mean field VB. Note that other choices can be made at each step leading to other inference methods (e.g. MCMC).\n",
    "    1. Use $q^*(z)$ to approximate $p(z|y)$\n",
    "    2. Optimization $q^* = argmin_{q \\in Q} f(q(z), p(z|y))$ ($f$ is some distance/divergence measure)\n",
    "    3. Variational Bayes $q^* = argmin_{q \\in Q} KL(q(z) || p(z|y))$\n",
    "    4. Mean field VB $q^* = argmin_{q \\in Q_{MFVB}} KL(q(z) || p(z|y))$\n",
    "    5. Coordinate ascent / gradient ascent (stochastic variational inference)\n",
    "* People mean the same thing with *variational bayes* and *variational inference* right?\n",
    "* Problems with MFVB:\n",
    "    * Problems with choice of KL divergence direction, missing modes or overcompensating.\n",
    "    * Problems with underestimation of variance.\n",
    "        * Many papers try to improve on this.\n",
    "* One way to evaluate seems to be to just compare to (longrunning) results of MCMC, if VI algorithm closely resembles MCMC posterior approximation it's good.\n",
    "* Bayesian coresets to scale to big data.\n",
    "* Bayesian coresets is basically a smarter way of subsampling a dataset to make it more scalable. In short I understood it as a way of picking representative datapoints and reweighting them to reflect their importance based on what they represent?\n",
    "    * Not just importance sampling.\n",
    "    * Points didn't necessarily have to be inside some cluster of data point as might seem intuitive? But rather they are picked to mimic the loss by using all data points.\n",
    "* Uniform subsampling might miss important points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Obfuscated Gradients, False Sense of Security\n",
    "* An adversarial example is something that is misclassified (with high confidence?) but it still very much looks like correctly classified examples. At least to a human.\n",
    "* Security threat, examples of this are misclassified traffic signs which is obviously a big problem for autonomous driving.\n",
    "* It's relatively easy to generate these. Speaker mentions different scenarios where the attacker has access to different parts of the network. Output logits, loss, just the topK labels etc, still manages to perform attacks.\n",
    "* Some try to obfuscate gradients but speaker argues this is a weak defense.\n",
    "* Speaker thinks more evaluation is needed and that many defense papers actually fail to evaluate their approaches properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conditional Neural Processes\n",
    "* Gaussian processes are nice because we can exploit prior knowledge and we get an uncertainty estimation but they are also expensive at test time. (Naive case is $\\mathcal{O}((n+m)^3)$ because of matrix inversion)\n",
    "* This paper tries to combine this with fast inference of neural networks by trading mathematical guarantees of GPs with scalability of NNs.\n",
    "* Essentially they have two networks, $h$ and $g$\n",
    "    * $h(x_i, y_i) = r_i$ for all training data input-output pairs\n",
    "    * Combine $r_i$ to embedding $r$ which functions as the knowledge of the behaviour of the function being regressed.\n",
    "    * Trained by maximizing log likelihood of predicted distributions for training data input-output pairs but conditioned on subsets of the training data (i.e. computing $r$ from subset?)\n",
    "        * Monte Carlo estimates by sampling from the predicted distributions.\n",
    "    * $\\phi_i = g(x_i, r)$ is the predicted distribution for test input $x_i$\n",
    "* One advantage: don't have to pick a prior (kernel for GPs?) but can learn this from empirical data samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fixing a Broken ELBO\n",
    "* This is concerning latent variable models trained by optimizing ELBO, like VAEs.\n",
    "* They argue that it doesn't necessarily give a good representation.\n",
    "* They show a curve of models with identical ELBO but with different tradeoff of compression and reconstruction ability. (Rate-Distortion tradeoff)\n",
    "    * E.g. we could have a model that simply discards the latent code and just draws samples from the prior. (KL collapse) Usually happens for very powerful decoders.\n",
    "* They say that mutual information between latent code Z and observed X is a better way to measure representation learning performance.\n",
    "* Takeaway, a more principled method for how to make a powerful decoder not ignore it's latent code.\n",
    "* Also see *TherML* related to this by same author."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tighter Variational Bounds are Not Necessarily Better\n",
    "* A lot of research in approximate inference tries to make the ELBO tighter.\n",
    "* This paper argues that this doesn't always give a better inference model.\n",
    "\n",
    "## Yes, But Did it Work? Evaluating Variational Inference\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Geometry Score\n",
    "* Generative models (and GANs in particular because implicit distribution) are very difficult to evaluate. A few methods exist but they are not perfect (inception score is sort of standard but critique against it is that it doesn't always correlate well with human judgement). Geometry score is another method.\n",
    "* Geometry score is not limited to visual data!\n",
    "* Geometry score works by comparing the underlying data manifolds for the data distribution and the generated distribution.\n",
    "    * No access to actual manifolds, just samples from them.\n",
    "    * Idea is based on Topological Data Analysis. Something like:\n",
    "        * If two points are within $\\epsilon$ by some distance measure then they are connected.\n",
    "        * This builds up the *simplicial complex* $\\mathcal{R}_\\epsilon$\n",
    "    * They build up simplicial complexes for different values of $\\epsilon$\n",
    "    * They then summarize these by counting the connected components?\n",
    "    * Which is then used for comparisons between manifolds.\n",
    "    * ??? lost some details here\n",
    "* Need to read paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Is Generator Conditioning Causally Related to GAN Performance?\n",
    "* Based on some insights from a previous paper stating that controlling the distribution of singular values in the Jacobian is important in deep learning, they try this on the generator in a GAN.\n",
    "* They test the hypothesis that ill-conditioned singular value distribution of the generator in a GAN leads to bad results.\n",
    "* They add a soft constraint to penalize very large and very small singular values.\n",
    "    * Sample $z$\n",
    "    * $z' = perturb(z)$\n",
    "    * Q = ||G(z) - G(z')|| / ||z - z'||\n",
    "    * $L_{penalty} = (max(Q, \\lambda_{max}) - \\lambda_{max})^2 + (min(Q, \\lambda_{min}) - \\lambda_{min})^2$\n",
    "* This added loss term gives more stable training and better inception and FID scores.\n",
    "* Relation to Spectral Normalization GAN? In that paper it's applied to discriminator and aims to normalize singular values of the *weight matrix*, leads to similar thing? But this paper also penalizes from below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Towards Binary-Valued Gates for Robust LSTM Training\n",
    "* LSTMs have \"gates\" to control information flow over time.\n",
    "* Authors want to \"binarize\" this, i.e. pushing them towards 0 or 1.\n",
    "* They state that this gives better generalization because of the flatter minima produced by the \"binarized\" gates.\n",
    "* They use the gumbel softmax reparametrization trick to accomplish this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Towards End-to-End Prosody Transfer for Expressive Speech Synthesis with Tacotron\n",
    "* Speech synthesis\n",
    "TODO: Read this paper\n",
    "\n",
    "## Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis\n",
    "* Speech synthesis with speaker style in latent variables.\n",
    "TODO: Read this paper. Related to above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Assessing Generative Models via Precision and Recall\n",
    "* They used precision and recall to evaluate GANs.\n",
    "\n",
    "## MMD GANs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
