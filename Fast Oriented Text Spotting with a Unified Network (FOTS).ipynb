{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fast Oriented Text Spotting with a Unified Network (FOTS)\n",
    "These slides concern both the paper and my implementation of it for a knowledge sharing session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overview\n",
    "\n",
    "<img src=\"figs/fots-overview.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data\n",
    "* In this paper (and other similar ones) the rotated bounding boxes are represented in the RBOX format.\n",
    "* An RBOX coordinate have 5 dimensions $(t, b, l, r, \\theta)$ which are distances to top, bottom, left, right and an angle.\n",
    "* An $(x,y)$ location coupled with a $(t, b, l, r, \\theta)$ rbox coordinate fully decribes a bounding box.\n",
    "* For FOTS the RBOX labels are needed at every pixel. We also need pixel-wise labels for whether a pixel is part of a text area or not.\n",
    "\n",
    "<img src=\"figs/fots-labels1.png\" width=\"45%\" align=\"left\"/>\n",
    "<img src=\"figs/fots-labels2.png\" width=\"45%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Shared Features\n",
    "* In the paper the shared features are computed by first applying a resnet 50 backbone network and then scaling up the feature maps from 1/32 to 1/4 the size of the original image size.\n",
    "* The upscaling is done with bilinear resizing.\n",
    "* It also combines lower level features with higher level features in a U-NET style.\n",
    "\n",
    "```python\n",
    "def build_shared_features_network(x, is_training):\n",
    "    with tf.variable_scope('backbone'):\n",
    "        x, block_groups = resnet_v1(data_format='channels_last')(x, is_training)\n",
    "\n",
    "    def deconv_merge(x, y):\n",
    "        h = tf.shape(y)[1]\n",
    "        w = tf.shape(y)[2]\n",
    "        c = y.shape[3]\n",
    "\n",
    "        x = tf.layers.conv2d(x, c, 3, 2, padding='same')\n",
    "        x = tf.image.resize_bilinear(x, (h, w))\n",
    "        x = tf.layers.batch_normalization(x, training=is_training)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = x + y\n",
    "\n",
    "        return x\n",
    "\n",
    "    x = deconv_merge(x, block_groups[-2])\n",
    "    x = deconv_merge(x, block_groups[-3])\n",
    "    x = deconv_merge(x, block_groups[-4])\n",
    "\n",
    "    return x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Text Detection Branch\n",
    "* The text detection branch makes pixel-wise (downscaled) predictions of whether a pixel is part of a text area (classification) and predictions of the RBOX coordinates of those pixels (bounding box regression).\n",
    "\n",
    "```python\n",
    "def build_text_detection_network(x, max_detection_dist, is_training):\n",
    "    conv = functools.partial(tf.layers.conv2d, padding='same')\n",
    "\n",
    "    score_map_logits = conv(x, 1, 1, 1)\n",
    "\n",
    "    dists = conv(x, 4, 1, 1)\n",
    "    dists = tf.nn.sigmoid(dists) * max_detection_dist\n",
    "    angle = conv(x, 1, 1, 1)\n",
    "    angle = tf.nn.tanh(angle) * math.pi / 2  # [-90, 90].\n",
    "    geom_map = tf.concat((dists, angle), axis=3)\n",
    "\n",
    "    return score_map_logits, geom_map\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Text Detection Loss\n",
    "* Sigmoid cross entropy for pixel classification loss.\n",
    "* $1 - cos(\\theta_{pred} - \\theta_{gt})$ for rotation regression loss.\n",
    "* Intersection over union loss for predicted rbox coordinates (without considering the rotation).\n",
    "* Mask loss from pixels that we don't care about.\n",
    "    * For classification we don't care about the pixels between the bounding box and the bounding box's corresponding \"shrunk area\".\n",
    "    * For the regression losses we don't care about the pixels outside bounding boxes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RoIRotate\n",
    "* RoIRotate is the operation that transforms the predicted text regions into axis-aligned feature maps that are then passed into the text recognition branch.\n",
    "* RoIRotate takes the shared featuremap along with a $(x,y)$ coordinate and a $(t, b, l, r, \\theta)$ RBOX geometry coordinate as input and returns an axis-aligned (unrotated) crop from the shared featuremap.\n",
    "* During training we use the ground truth locations and geometries of text regions as input to RoIRotate when extracting the axis-aligned feature maps to be used with the text recognition branch.\n",
    "\n",
    "<img src=\"figs/fots-roi-rotate-visualization.png\" width=\"30%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How it works\n",
    "* Essentially, for each pixel location in the extracted axis-aligned feature map we need to know which pixel location in the source feature map (the shared feature space) to look at and sample from.\n",
    "* Thus we need a mapping between these coordinate systems.\n",
    "$$\n",
    "\\mathbf{M}\n",
    "\\begin{pmatrix}\n",
    "    x_s \\\\\n",
    "    y_s \\\\\n",
    "    1\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "    x_t \\\\\n",
    "    y_t \\\\\n",
    "    1\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "* $\\mathbf{M}$ is constructed from the location in shared feature space, the RBOX geometry and a target height of the cropped out feature map.\n",
    "* $\\mathbf{M}$ encodes a series of transformations to map between these coordinate systems.\n",
    "* When cropping out the axis aligned feature map, we need $\\mathbf{M}^{-1}$ because we know the target locations and need to figure out the source locations.\n",
    "* In practice, it's possible (and maybe more stable / faster ?) to construct $\\mathbf{M}^{-1}$ directly by taking the inverse of each separate transformation matrix which is easy because they're just rotations, translations and scaling matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Locality Aware NMS\n",
    "* Since we have dense predictions with FOTS, there will be a lot of candidate bounding boxes for every actual box.\n",
    "* NMS is a way to merge together bounding boxes if they are deemed similar enough, i.e. if they have an IoU over a certain threshold.\n",
    "* This is done iteratively by going through the candidate boxes row by row.\n",
    "* In another paper (EAST) they propose a small change to NMS to make it a bit faster which is what is implemented here.\n",
    "* The current implementation is in tensorflow, but possibly there are performance gains to be had by rewriting it as a custom c++ op.\n",
    "* The current implementation also uses a general algorithm for computing intersection areas between two polygons. In this application we will always have convex quadrilaterals for which there might be faster algorithms.\n",
    "\n",
    "<img src=\"figs/fots-nms.png\" width=\"65%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Text Recognition Branch\n",
    "* The text recognition branch takes the axis-aligned feature maps and first applies a cnn with heightwise pooling layers to compute a height=1 sequence feature map.\n",
    "* This feature map is then fed into a two layer bidirectional LSTM.\n",
    "* The computed LSTM states are summed and fed into a dense classification layer for framewise predictions.\n",
    "\n",
    "```python\n",
    "def build_text_recognition_network(x, widths, charset_size, dropout_rate, is_training):\n",
    "    def conv_bn_relu(x, d, k, s):\n",
    "        x = tf.layers.conv2d(x, d, k, s, padding='same')\n",
    "        x = tf.layers.batch_normalization(x, training=is_training, axis=3)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x\n",
    "\n",
    "    def height_max_pool(x):\n",
    "        return tf.layers.max_pooling2d(x, (2, 1), (2, 1), padding='valid')\n",
    "\n",
    "    x = conv_bn_relu(x, 64, 3, 1)\n",
    "    x = conv_bn_relu(x, 64, 3, 1)\n",
    "    x = height_max_pool(x)\n",
    "    x = conv_bn_relu(x, 128, 3, 1)\n",
    "    x = conv_bn_relu(x, 128, 3, 1)\n",
    "    x = height_max_pool(x)\n",
    "    x = conv_bn_relu(x, 256, 3, 1)\n",
    "    x = conv_bn_relu(x, 256, 3, 1)\n",
    "    x = height_max_pool(x)\n",
    "    x = tf.squeeze(x, axis=1)\n",
    "    \n",
    "    ...\n",
    "    \n",
    "    (x_fw, x_bw), _ = tf.nn.bidirectional_dynamic_rnn(\n",
    "        tf.contrib.cudnn_rnn.CudnnCompatibleLSTMCell(256),\n",
    "        tf.contrib.cudnn_rnn.CudnnCompatibleLSTMCell(256),\n",
    "        x,\n",
    "        sequence_length=sequence_lengths,\n",
    "        dtype=tf.float32)\n",
    "    \n",
    "    x = x_fw + x_bw\n",
    "    x = tf.layers.dropout(x, rate=dropout_rate, training=is_training)\n",
    "    x = tf.layers.dense(x, charset_size + 1)  # Last label for blank ctc prediction.\n",
    "\n",
    "    return x, sequence_lengths\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Text Recognition Loss\n",
    "* The paper uses CTC loss.\n",
    "* CTC loss is a way to compute a loss (and make predictions) for a sequence of predictions where the labels are unaligned. I.e. the prediction is a sequence and label is another sequence but they can be of different lengths and we don't which label should correspond to which prediction.\n",
    "* This is good because text comes in many different shapes and forms and each frame might have different levels of information.\n",
    "* CTC adds a blank token and then (intuitively) considers different sequences of actual tokens and the blank token which will see which alignment of predictions and labels give the best match."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Text Recognition Prediction\n",
    "* Similarly, the paper uses CTC beam search decoding.\n",
    "* E.g. for a very widely stretched 'a' there might be many predictions in the prediction sequence saying 'a' but in the output sequence it should only be one.\n",
    "* The beam search then considers many such sequences to make the final best prediction sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluation\n",
    "* Currently I'm computing the precision and recall (and f-measure) to measure the performance of the text detection. Note that this recall metric should not be confused with the recall measured where the model was used in conjunction with the regexes defined for the Corotos marketplace. \n",
    "* Hits for precision and recall of text detection are defined (in most papers) as having an iou > 0.5 with it's ground truth box. I've done the same in this case.\n",
    "    * There are some complexities involved with this discussed on the next subslide.\n",
    "* For text recognition, i.e. predicting the text in a bounding box, I'm evaluating this with normalized Levenshtein edit distance.\n",
    "* The normalized edit distance is probably what we want to use as the metric to look at to guide experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Difficulty\n",
    "* The final predicted bounding boxes and the ground truth bounding boxes are both in an undefined order.\n",
    "* In order to compute the previously mentioned metrics, we need to first match predictions with ground truths.\n",
    "* This is a combinatorial optimization problem and I've done this with the *Hungarian algorithm* based on IoUs between each prediction ground truth pair.\n",
    "    * TODO: Add image explaining it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Current State / Discussion\n",
    "* We have little real world data to evaluate on.\n",
    "    * None if we consider the metrics dealing with text detection.\n",
    "* The current evaluation task is based on how good the model plus some regexes is at finding matches.\n",
    "* For the data we have generated, the model reaches f-measures over 0.9.\n",
    "    * However, this might be considered too easy since the it's only a random split rather than for example keeping certain fonts out of the training set and then evaluating on images containing these.\n",
    "    * But it probably says something about what the model is capable of.\n",
    "* On actual ad images, qualitatively, it's seems a bit worse. But more analysis is needed.\n",
    "* I think there are \"quick\" wins by improving how we generate data by trying to mimic real world data better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
