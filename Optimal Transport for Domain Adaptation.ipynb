{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal Transport for Domain Adaptation\n",
    "\n",
    "## Introduction\n",
    "* Why\n",
    "    * The distribution of data used to learn a decision function (classifier) and the data used at test time are not always the same.\n",
    "        * E.g. Change in lighting condition, different sensor type, male/female voices\n",
    "        * A.k.a. *drift* between train and test distributions\n",
    "* What\n",
    "    * Want to learn to be able to transfer knowledge from one domain (source) to another (target) and still do well in some task. This is called *domain adaptation*.\n",
    "        * Semi-supervised domain adaptation, labels in source domain and a few in target domain.\n",
    "        * Unsupervised domain adaptation, labels only in source domain.\n",
    "    * Want to learn a mapping $T$ from source domain to target domain.\n",
    "    * Optimal Transport (OT) is one way to approach this.\n",
    "    * In non domain adaptation we try to find a common subspace (generalization?)\n",
    "\n",
    "## Domain Adaptation Problem Formulation\n",
    "* $\\Omega \\in \\mathbb{R}^d$ is data, $\\mathcal{C}$ is set of possible labels.\n",
    "    * $\\Omega_s$, $\\Omega_t$ are source and target domains\n",
    "* $\\mathcal{P}(\\Omega)$ is set of all probability measures over $\\Omega$ \n",
    "* Two joint distributions over data in two separate domains and their labels\n",
    "    * $P_s(x^s, y)$ and $P_t(x^t, y)$\n",
    "* $\\mu_s, \\mu_t$ are marginal distributions over X (note: X encompasses both omegas?)\n",
    "* $f_s, f_t$ are true labeling functions\n",
    "* Assumption: $T: \\Omega_s \\to \\Omega_t$ potentially nonlinear transformation.\n",
    "    * $P_s(y|x^s) = P_t(y|T(x^s))$ which means $f_t(T(x)) = f_s(x)$, i.e. same label.\n",
    "    * Another view: $X_t$ are drawn from the same distributions as $T#\\mu_s$\n",
    "\n",
    "        \n",
    "# TODO:\n",
    "* problem definition as per the paper\n",
    "* legend over all symbols\n",
    "* side notes about math stuff\n",
    "* generalized conditional gradient splitting\n",
    "\n",
    "## Experiments\n",
    "* Four models, TODO: explain these elsewhere\n",
    "    * Exact OT\n",
    "    * Information theoretic regularization\n",
    "    * Class label regularized from paper\n",
    "    * Laplace regularized from paper\n",
    "\n",
    "* Two moons\n",
    "    * Source domain is two entangled moon crescents (two classes), targets is rotated versions of this.\n",
    "    * Fixed classifier (svm with gaussian kernel), params chosen from cross validation.\n",
    "    * note: are the support vectors translated? or how does this work?\n",
    "* Digits\n",
    "* Face recognition\n",
    "* Object recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
