{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Survey of Neural Abstractive Text Summarization\n",
    "Summary of a summary\n",
    "https://arxiv.org/abs/1812.02303"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluation metrics\n",
    "\n",
    "### ROUGE (Recall-Oriented Understudy for Gisting Evaluation)\n",
    "* Most common metric for summarization tasks.\n",
    "* Recall based.\n",
    "* A few different versions. Commonly reported are:\n",
    "    * **ROUGE-N** measures overlap of n-grams (usually N=1, or N=2).\n",
    "    * **ROUGE-L** measures longest matching subsequence (not necessarily consecutive).\n",
    "* Critique:\n",
    "    * Only cares about content, not about grammatical correctness, fluency, coherence, etc.\n",
    "    * Content evaluation is perhaps a bit biased against abstractive approached. Consider synonyms for example.\n",
    "    * Sort of designed to be evaluated with multiple reference summaries but common datasets only have one per input text.\n",
    "\n",
    "### Others\n",
    "* METEOR\n",
    "* BLEU\n",
    "* PPL\n",
    "* CIDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Seq2Seq models\n",
    "* Sequence to Sequence, i.e. a model that takes a sequence (of tokens) as input and outputs another sequence.\n",
    "* Not necessarily the same length.\n",
    "* Used in most (all?) modern abstractive summarization tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Basics\n",
    "* Modeled as an **encoder** and a **decoder**.\n",
    "* Encoder takes the input sequence $x = (x_1, \\ldots, x_J)$ and outputs hidden states $h^e = (h_1^e, \\ldots, h_J^e)$\n",
    "* Decoder takes the hidden states and outputs output sequence $y = (y_1, \\ldots, y_T)$\n",
    "* In this case the input tokens are the tokens in the text to be summarized and the output tokens are the tokens of the summary text.\n",
    "\n",
    "<img src=\"figs/nats-fig-2.png\" width=\"50%\">\n",
    "\n",
    "* Encoders and decoders can be implemented as MLPs, CNNs, RNNs.\n",
    "* The figure shows a bidirectional LSTM encoder and an LSTM decoder.\n",
    "* In the LSTM case, usually the final hidden state of each direction is concatenated and used as the initial hidden state of the decoder, $h_0^d$. Same for initial cell state $c_0^d$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Problems\n",
    "* It's difficult to train the encoder because there are many computational steps between loss from a predicted token and the responsible part of the encoder meaning limited propagation of the gradient signal.\n",
    "* Representational difficulties: everything needs to be remembered in the final hidden state(s).\n",
    "* Repetition in generated output.\n",
    "* Factual errors in generated output.\n",
    "\n",
    "The following slides are some attempts to improve on these shortcomings (from a bunch of different papers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Attention\n",
    "* The goal of attention is to reduce the distance of information flow so that the model can get the relevant information more easily.\n",
    "    * Alleviates the gradient propagation problem.\n",
    "    * Less dependent on storing everything in the hidden state / context fed into the decoder.\n",
    "* With attention, the decoder is allowed to look at all encoder hidden states $h_j^e$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Attention is just a distribution over the source tokens corresponding hidden states that is then taken into account when making token predictions and computing the next hidden states.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    s_{tj}^e &= s(h_j^e, h_t^d) \\\\\n",
    "    \\\\\n",
    "    \\alpha_{tj}^e &= \\frac{exp(s_{tj}^e)}{\\sum_{k=1}^J exp(s_{tk}^e)} \\\\\n",
    "    \\\\\n",
    "    z_t^e &= \\sum_{j=1}^J \\alpha_{tj}h_t^e \\\\\n",
    "    \\\\\n",
    "    \\tilde{h}_t^d &= \\mathbf{W}_z(z_t^e \\oplus h_t^d) + b_z \\\\\n",
    "    \\\\\n",
    "    P_{vocab,t} &= softmax(\\mathbf{W}_{d2v}\\tilde{h}_t^d + b_{d2v})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "* Choice of scoring function $s(h_j^e, h_t^d)$ has some different alternatives which may include extra learnable parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* The figure shows the same bidirectional LSTM encoder and an LSTM decoder with attention at a certain decoding step.\n",
    "<img src=\"figs/nats-fig-3.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Transformer\n",
    "* Network architecture from \"Attention is all you need\" paper, very common in summarization papers since.\n",
    "* No RNN or CNN, instead just relying on attention.\n",
    "* Can be used for both encoder and decoder.\n",
    "* Encoder is made up of several encoding layers where each such layer can attend to all positions in the current (layerwise) encoded sequence (self-attention).\n",
    "* Decoder is made up of several decoding layers where each such layer can \n",
    "    * attend to all positions in the final encoded sequence.\n",
    "    * attend to all previously generated positions in the decoded sequence (i.e. future is masked)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Pointing/Copying mechanism\n",
    "* The goal of these approaches is to handle out-of-vocabulary words.\n",
    "    * e.g. a named entity that is not part of the vocabulary and couldn't otherwise be part of the summary.\n",
    "* The following approaches all seem very similar to each other differing mainly in \"hard\" or \"soft\" switches when choosing between generating and copying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Pointer softmax\n",
    "Three parts in play at each decoding step $t$.\n",
    "\n",
    "* Softmax prediction over vocabulary.\n",
    "* Softmax prediction over locations in input sequence based on attention weights $\\alpha_{tj}^e$.\n",
    "* Switching network (MLP+sigmoid) to softly pick token from either predicted vocabulary distribution or input sequence by concatenating the weighted distributions.\n",
    "$$\n",
    "\\begin{align*}\n",
    "    p_{gen,t} = \\sigma(\\mathbf{W}_{s,z}z_t^e + \\mathbf{W}_{s,h}h_t^d + b_s)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Switching Generator-Pointer\n",
    "Seems similar to Pointer softmax but more explicitly choosing argmax from either predicted vocabulary distribution OR input sequence based on switch defined below.\n",
    "$$\n",
    "\\begin{align*}\n",
    "    p_{gen,t} = \\sigma(\\mathbf{W}_{s,z}z_t^e + \\mathbf{W}_{s,h}h_t^d + \\mathbf{W}_{s,E}E_{y_{t-1}}b_s)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Copynet\n",
    "* Extended vocabulary $\\mathcal{V}_{ext}$ from union of vocabulary and input sequence.\n",
    "* One distribution for generating from vocabulary and one distribution for copying from input sequence.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    P_{\\mathcal{V}_{ext}} &= P_g(y_t) + P_c(y_t) \\\\\n",
    "    \\\\\n",
    "    P_g(y_t) &= \\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            \\frac{1}{Z}exp(\\psi_g(y_t)) & \\quad y_t \\in \\mathcal{V} \\\\\n",
    "            0 & \\quad otherwise\n",
    "        \\end{array}\n",
    "    \\right. \\\\\n",
    "    \\\\\n",
    "    P_c(y_t) &= \\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            \\frac{1}{Z}\\sum_{x_j=y_t} exp(\\psi_c(x_j)) & \\quad y_t \\in \\mathcal{X} \\\\\n",
    "            0 & \\quad otherwise\n",
    "        \\end{array}\n",
    "    \\right. \\\\\n",
    "    \\\\\n",
    "    \\psi_g(y_t) &= \\mathbf{W}_{d2v}\\tilde{h}_t^d + b_{d2v} \\\\\n",
    "    \\\\\n",
    "    \\psi_g(y_t) & \\text{ from score function}\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Pointer-Generator network\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    P_g(y_t) &= \\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            P_{vocab,t}(y_t) & \\quad y_t \\in \\mathcal{V} \\\\\n",
    "            0 & \\quad otherwise\n",
    "        \\end{array}\n",
    "    \\right. \\\\\n",
    "    \\\\\n",
    "    P_c(y_t) &= \\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            \\sum_{x_j=y_t} \\alpha_{tj}^e & \\quad y_t \\in \\mathcal{X} \\\\\n",
    "            0 & \\quad otherwise\n",
    "        \\end{array}\n",
    "    \\right. \\\\\n",
    "    \\\\\n",
    "    P_{\\mathcal{V}_{ext}}(y_t) &= p_{gen,t}P_g(y_t) + (1 - p_{gen,t})P_c(y_t)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Repetition handling\n",
    "* The goal here is to reduce repetition in the generated sequence which is a common problem. This occurs both at word level and at sentence level.\n",
    "* The general idea is to in some way improve how the model can remember what has been generated before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Coverage\n",
    "* The aim is to remember what has been attended before.\n",
    "* Define a coverage vector $u_t^e$ as the sum of previous attention distributions.\n",
    "* Add coverage vector as input to attention score function to allow the current decoding step to know about previous attentions.\n",
    "* Add a coverage loss $\\sum_j min(\\alpha_{tj}^e, u_{tj}^e)$ so that attention is not put at the same place repeatedly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Temporal attention\n",
    "Similar to coverage but looks at the attention scores $s_{tj}^e$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    s_{tj}^{temp} &= \\frac{exp(s_{tj}^e)}{\\sum_k^{t-1} exp(s_{kj}^e)} \\\\\n",
    "    \\\\\n",
    "    \\alpha_{tj}^{temp} &= \\frac{s_{tj}^{temp}}{\\sum_k^J s_{tk}^{temp}} \\\\\n",
    "    \\\\\n",
    "    z_t^e &= \\sum_j^J \\alpha_{tj}^{temp} h_j^e\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Intra-decoder attention\n",
    "* Similar to temporal attention (and coverage) but introduces $s_{t\\tau}^d$ which is based on attention scores within the decoder only.\n",
    "* A decoder side context vector $z_t^d$ is computed and is used in addition to the encoder side $z_t^e$ to compute $P_{vocab}$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    s_{t\\tau}^d &= s(h_\\tau^d, s_t^d) \\\\\n",
    "    \\\\\n",
    "    \\alpha_{t\\tau}^d &= \\frac{exp(s_{t\\tau}^d)}{\\sum_k^{t-1} exp(s_{tk}^d)} \\\\\n",
    "    \\\\\n",
    "    z_t^d &= \\sum_\\tau^{t-1} \\alpha_{t\\tau}^d h_\\tau^d\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Distraction\n",
    "* Similar to coverage in that each decoding step should know about what has been what has focused on in previous decoding step.\n",
    "* In this case this is based on the previous context vectors.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    z_t^{dist,t} = tanh(\\mathbf{W}_{dist,z} z_t^e - \\mathbf{W}_{hist,z} \\sum_j^{t-1} z_j^e)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Improving encoded representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Selective encoding\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Read-again encoding\n",
    "* Basically pass the sequence through the first encoder once to get the hidden states $(h_1^{e,1}, \\ldots, h_J^{e,1})$.\n",
    "* Then add another encoder whose hidden state is updated according to \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    h_{sen}^{e,1} &= h_J^{e,1} \\\\\n",
    "    \\\\\n",
    "    h_j^{e,2} &= LSTM(h_{j-1}^{e,2}, E_{x_j} \\oplus h_j^{e,1} \\oplus h_{sen}^{e,1})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "* Hidden states of the second read is then used by decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Improving decoder\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summarizing long documents\n",
    "* Want to capture hierarchical representation of longer documents.\n",
    "* \"Chunks\" refers to higher level representation.\n",
    "* Essentially, use one encoder at token level and one at chunk level and use information from both at decoding steps.\n",
    "* At decoding time $t$ we have both chunk attention weights $\\alpha_i^{chk,t}$ and token attention weights $\\alpha_{ij}^{wd,t}$ computed from hidden states from chunk and token hidden states respectively.\n",
    "\n",
    "<img src=\"figs/nats-fig-7.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Hierarchical attention\n",
    "Tokens in less important chunks should be less attended so computation of the context vector is changed.\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\alpha_{ij}^{scale,t} &= \\frac{\\alpha_i^{chk,t}\\alpha_{ij}^{wd,t}}{\\sum_{k,l}\\alpha_k^{chk,t}\\alpha_{kl}^{wd,t}} \\\\\n",
    "    \\\\\n",
    "    z_t^e &= \\sum_{i,j} \\alpha_{ij}^{scale,t}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Discourse-aware attention \n",
    "Similar to hierarchical attention, but slightly different computation of scaled attention weights.\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\alpha_{ij}^{scale,t} &= \\frac{exp(\\alpha_i^{chk,t}s_{ij}^{wd,t})}{\\sum_{k,l} exp(\\alpha_k^{chk,t}s_{kl}^{wd,t})} \\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Course-to-fine attention\n",
    "Also similar to hierarchical attention but computes context vector by sampling chunk $i$ and then using only token level hidden states for that chunk instead of all chunks.\n",
    "$$\n",
    "\\begin{align*}\n",
    "    z_t^e &= \\sum_j \\alpha_{ij}^{scale,t}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Graph-based attention\n",
    "* Different from hierarchical attention in that instead implicitly capturing which chunks are important this is done explicitly by constructing a graph where chunks are vertices and similarities are edges and applying pagerank.\n",
    "    * (Skipping details)\n",
    "* Intuition: Pagerank score of a vertex increases with more edges pointing to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Extraction + Abstraction\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training strategies\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary generation\n",
    "* Output of the decoder at each decoding step is a distribution over tokens.\n",
    "* At prediction time we want to pick a strategy for using these distribution to produce a sequence that is close to optimal with regards to likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Beam search\n",
    "* Pick beam size $B$\n",
    "* We always keep track of the top $B$ states of the decoder and sequence produced so far. This includes internal (LSTM) states etc.\n",
    "* At each decoding step we consider picking the top $B$ tokens and \"locking\" them in in $B$ separate cases.\n",
    "* With it each token locked in and passed to the next hypothetical LSTM state we consider that likelihood of the next potential tokens after it.\n",
    "* For $B=5$ and vocabulary size 10000, we consider 5 * 10000 cases and then pick the top 5 of these.\n",
    "* Repeat this process until done.\n",
    "* $B=1$ gives greedy search, i.e. just picking most likely at each time step. But usually this is inferior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Diverse beam search\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recent papers\n",
    "TODO: Include some produced summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Discussion points\n",
    "* ROUGE is not so good, could hinder progress. Did anyone look at the others?\n",
    "* Many approaches to solving some of the issues were essentially the same idea applied at different layers. Pros and cons of some of them?\n",
    "* Is this useful to us? Right now or can be in the future. Are results good enough to make a product?\n",
    "* Did anyone look at the reinforcement learning approaches?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
