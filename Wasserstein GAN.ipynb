{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Wasserstein GAN\n",
    "\n",
    "This paper is concerned about measuring how close the model distribution is to the real distribution.\n",
    "\n",
    "Practically, I think the main contributions are\n",
    "* It presents another way to stabilize training of GANs which seems to work really well.\n",
    "* There seems to be a correlation between discriminator loss and quality of the generated content.\n",
    "\n",
    "The paper is relatively theory heavy, which gives a theoretical reason for why this new algorithm works, but the changes needed are also very easy to implement.\n",
    "\n",
    "\n",
    "## Background\n",
    "The paper begins with some background on learning distributions, i.e. generative models. Using their notation:\n",
    "* $P_r$ is the actual distribution\n",
    "* $P_\\theta$ is the model distribution parametrized by $\\theta$.\n",
    "\n",
    "The most intuitive way of finding $P_\\theta$ might be the maximum likelihood estimation (MLE) method $\\ argmax_\\theta \\frac{1}{m} \\sum^m_{i=1} log\\ P_\\theta (x^{(i)})$\n",
    "\n",
    "In the limit $m \\to \\infty$ MLE is equal to minimizing the KL divergence $KL(P_r\\ ||\\ P_\\theta) = \\int TODO$\n",
    "\n",
    "This means that the KL divergence will go to $\\infty$ if even just a single datapoint gets zero density under $P_\\theta$. We are often dealing with data on a lower dimensional manifold, and then it will be very unlikely that all mass of $P_r$ that of $P_theta$ covers each other perfectly which in turn means that the KL divergence will go to infinity. Bad!\n",
    "\n",
    "MLE is still used a lot, but to solve this issue a noise term is usually added to $P_\\theta$ to ensure that all data points have some density. This method also has drawbacks however since it reduces quality (blurriness).\n",
    "\n",
    "Another approach to find a good $P_\\theta$ is to learn a deterministic function $g_\\theta:\\ z \\rightarrow x$ (e.g. a neural network) where $z$ is sampled from a known distribution $p(z)$.\n",
    "\n",
    "For this approach, a measure of closeness between distributions is needed. The paper discusses some different such measures.\n",
    "* Total Variation (TV)\n",
    "\n",
    "\n",
    "\n",
    "## TODO\n",
    "Lipchitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
