{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do GANs actually learn the distribution? An empirical study\n",
    "\n",
    "**TLDR**\n",
    "TODO\n",
    "\n",
    "## Introduction\n",
    "The original GAN paper says that with a \"sufficiently large\" generator and sample size, training will converge arbitrarily close to the target distribution (measured with Jensen Shannon divergence). This paper explores how well it works under more realistic conditions, i.e. realistic sample sizes. They also introduce an empirical test for support size of the learned distribution.\n",
    "\n",
    "Previously methods for testing this have been\n",
    "* Checking whether training set has just been memorized.\n",
    "* Check similarity of each generated image to nearest images in training set.\n",
    "* Interpolate between two random latent codes $z_1, z_2$ and check that images generated from latent codes lying on the interpolation line are reasonable and original.\n",
    "* Check for semantically meaningful directions in latent space, i.e. varying latent codes along these directions should produce predictable changes.\n",
    "* Annealed Importance Sampling to check the log-likelihoods of GANs.\n",
    "\n",
    "They also build upon results from a previous paper (*Generalization and Equilibrium in Generative Adversarial Nets*) which showed \n",
    "* Discriminator of size $n$ means support increases according to $\\mathcal{O}(n\\ log\\ n / \\epsilon^2)$ where $\\epsilon$ is closeness to optimal training objective.\n",
    "\n",
    "\n",
    "## Birthday paradox\n",
    "* In a discrete distribution of support $N$ the birthday paradox says that a sample of size $\\sqrt{N}$ is \"quite likely\" to have a collision/duplicate.\n",
    "    * For actual birthdays, it's about 38%\n",
    "    * What about non uniform distributions?\n",
    "    \n",
    "## Test for support size\n",
    "They suggest the following method based on the birthday paradox:\n",
    "* Sample $s$ from learned generative distribution\n",
    "* Use some image similarity measure to flag the closest 20 (arbitrary) pairs\n",
    "* Visually inspect and check for duplicates\n",
    "* Repeat\n",
    "\n",
    "If this test often has duplicates, then we can say support size is about $s^2$.\n",
    "\n",
    "However, the generative distribution can have a non uniform distribution in which case it doesn't work so well but this is a failure mode of GANs as well.\n",
    "\n",
    "For GANs we are working with continuous distributions, but they say that this test still works if the steps described above are taken. I.e. we visually determine if a collision has happened.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
