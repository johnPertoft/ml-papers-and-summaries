{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "Simple implementation and explanation of backward propagation using linear layers with some simple activation functions that I did when studying this initially.\n",
    "\n",
    "Backpropagation computes the gradients of the parameters in a computational graph. Backpropagation is the chain rule of differentiation where we are able to reuse computed partial differentials by stepping backwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_gaussian_quantiles\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0 / (1. + np.exp(-z))\n",
    "\n",
    "def d_sigmoid(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "def sigmoid_backwards(a):\n",
    "    return a * (1 - a)\n",
    "\n",
    "SIGMOID_ACTIVATION = (sigmoid, sigmoid_backwards)\n",
    "\n",
    "\"\"\"\n",
    "def tanh(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "def d_tanh(z):\n",
    "    pass\n",
    "\n",
    "TANH_ACTIVATION = (tanh, d_tanh)\n",
    "\"\"\"\n",
    "\n",
    "def relu(z):\n",
    "    return np.max(0, z)\n",
    "\n",
    "def d_relu(z):\n",
    "    return 1.0 * (z > 0)\n",
    "\n",
    "RELU_ACTIVATION = (relu, d_relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_parameters(layer_shapes):\n",
    "    \"\"\"Create initial random parameters.\"\"\"\n",
    "    parameters = {}\n",
    "    for i, shape in enumerate(layer_shapes):\n",
    "        parameters[\"W{}\".format(i+1)] = np.random.randn(*shape) * 0.01\n",
    "        parameters[\"b{}\".format(i+1)] = np.zeros((1, shape[1]))\n",
    "    return parameters\n",
    "        \n",
    "def forward(X, parameters, activation_fns):\n",
    "    \"\"\"Compute the activations at each layer.\"\"\"\n",
    "    A = X\n",
    "    activations = [X]\n",
    "    for i, (activation_fn, _) in enumerate(activation_fns):\n",
    "        W = parameters[\"W{}\".format(i+1)]\n",
    "        b = parameters[\"b{}\".format(i+1)]\n",
    "        A = activation_fn(A @ W + b)\n",
    "        activations.append(A)\n",
    "    return activations\n",
    "\n",
    "def backward(parameters, activation_fns, activations, dA_loss):\n",
    "    \"\"\"Compute gradients for all parameters.\"\"\"\n",
    "    gradients = {}\n",
    "    dA = dA_loss\n",
    "    for l in reversed(range(1, len(activations))):\n",
    "        W = parameters[\"W{}\".format(l)]\n",
    "        b = parameters[\"b{}\".format(l)]\n",
    "        A = activations[l]\n",
    "        m = A.shape[0]\n",
    "        A_prev = activations[l-1]\n",
    "        _, diff_activation_fn = activation_fns[l-1]  # TODO: Make this consistent with naming of parameters.\n",
    "        dZ = dA * diff_activation_fn(A)\n",
    "        dW = (1 / m ) * A_prev.T @ dZ\n",
    "        db = np.mean(dZ, axis=0, keepdims=True)\n",
    "        gradients[\"W{}\".format(l)] = dW\n",
    "        gradients[\"b{}\".format(l)] = db\n",
    "        dA = dZ @ W.T\n",
    "    return gradients\n",
    "    \n",
    "def update(parameters, gradients, learning_rate):\n",
    "    \"\"\"Compute the next set of parameters given the gradients and learning rate.\"\"\"\n",
    "    return {k: v - learning_rate * gradients[k] for k, v in parameters.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model example: binary classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAD8CAYAAAC2PJlnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5e1c0e1048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45\n"
     ]
    }
   ],
   "source": [
    "def binary_classifier(X_train, Y_train, X_test, Y_test, learning_rate, train_steps):\n",
    "    input_dim = X.shape[1]\n",
    "    layer_shapes = [(input_dim, 10), (10, 5), (5, 1)]\n",
    "    activation_fns = [SIGMOID_ACTIVATION, SIGMOID_ACTIVATION, SIGMOID_ACTIVATION]\n",
    "    assert len(layer_shapes) == len(activation_fns)\n",
    "    \n",
    "    parameters = init_parameters(layer_shapes)\n",
    "    \n",
    "    def cross_entropy_loss(A, Y):\n",
    "        \"\"\"Compute the cross entropy loss.\"\"\"\n",
    "        m = A.shape[0]\n",
    "        return (-(1 / m) * (np.log(A).T @ Y + np.log(1 - A).T @ (1 - Y))).squeeze()\n",
    "    \n",
    "    # TODO: numerical issues with this?\n",
    "    def d_cross_entropy_loss(A, Y):\n",
    "        \"\"\"Compute the derivative of the cross entropy loss w.r.t. the net's output.\"\"\"\n",
    "        m = A.shape[0]\n",
    "        return -Y / A + (1 - Y) / (1 - A)\n",
    "    \n",
    "    # Train the network.\n",
    "    losses = np.zeros(train_steps)\n",
    "    for i in range(train_steps):\n",
    "        activations = forward(X_train, parameters, activation_fns)\n",
    "        A = activations[-1]\n",
    "        losses[i] = cross_entropy_loss(A, Y_train)\n",
    "        dA_loss = d_cross_entropy_loss(A, Y_train)\n",
    "        gradients = backward(parameters, activation_fns, activations, dA_loss)\n",
    "        parameters = update(parameters, gradients, learning_rate)\n",
    "    \n",
    "    def predict(X):\n",
    "        \"\"\"Closure to predict with the trained network.\"\"\"\n",
    "        activations = forward(X, parameters, activation_fns)\n",
    "        output = activations[-1]\n",
    "        return output > 0.5\n",
    "    \n",
    "    return predict, losses\n",
    "\n",
    "N = 1000\n",
    "X, Y = make_gaussian_quantiles(n_classes=2, n_features=2, n_samples=N)\n",
    "Y = Y[:, np.newaxis]\n",
    "split_idx = int(0.8 * N)\n",
    "X_train = X[:split_idx]\n",
    "Y_train = Y[:split_idx]\n",
    "X_test = X[split_idx:]\n",
    "Y_test = Y[split_idx:]\n",
    "\n",
    "predict, losses = binary_classifier(X_train,\n",
    "                                    Y_train,\n",
    "                                    X_test,\n",
    "                                    Y_test,\n",
    "                                    learning_rate=0.05,\n",
    "                                    train_steps=1000)\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.show()\n",
    "Y_pred = predict(X_test)\n",
    "accuracy = (Y_pred.astype(int) == Y_test).mean()\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "binary_classifier() missing 4 required positional arguments: 'X_test', 'Y_test', 'learning_rate', and 'train_steps'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-367f15953f67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# TODO train/test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mbinary_classifier_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbinary_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: binary_classifier() missing 4 required positional arguments: 'X_test', 'Y_test', 'learning_rate', and 'train_steps'"
     ]
    }
   ],
   "source": [
    "X = None\n",
    "Y = None\n",
    "# TODO train/test set\n",
    "\n",
    "binary_classifier_predict = binary_classifier(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
