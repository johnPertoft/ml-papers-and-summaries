{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Video Representation Learning by Dense Predictive Coding (DPC)\n",
    "\n",
    "- Self-supervised technique for learning spatio-temporal representations of video.\n",
    "- For example, these features can be used in a downstream action recognition task.\n",
    "- Hopefully also suitable for capturing features that can be used in our pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n",
    "- Video data lends itself pretty well to self-supervised learning techniques.\n",
    "- There's lots of it (youtube, etc).\n",
    "- Commonly self-supervised techniques in this domain are based on predicting future frames in a video.\n",
    "- The main difficulty in this lies in that the future is very much non deterministic.\n",
    "- If the goal is just to learn a representation for downstream tasks then it is likely unnecessary to waste model capacity on predicting pixel level data of future frames.\n",
    "- In this paper these future predictions are instead made in a latent representation space.\n",
    "- The DPC model is similar to techniques like contrastive predictive coding (CPC) and word2vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DPC overview\n",
    "- DPC is designed to predict the future latent representations based on the recent past.\n",
    "- DPC is trained using a variant of noise contrastive estimation which means that it never has to predict the exact future but rather just has to solve the multi-class classification problem of picking the correct future state out of many distractors.\n",
    "- To do this the model has to learn to capture the shared semantics of multiple future states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DPC framework\n",
    "- The input video is partitioned into multiple non-overlapping blocks $x_1, ..., x_n$ which each consists of a fixed number of frames.\n",
    "    - $x_t \\in \\mathbb{R}^{T \\times H \\times W \\times C}$\n",
    "- Each block is then encoded into the latent representations $z_1, ..., z_n$ via an encoding function $f$.\n",
    "    - $z_t \\in \\mathbb{R}^{H' \\times W' \\times C}$\n",
    "- An aggregation function $g$ computes a context representation $c_t = g(z_1, ..., z_t)$ given the observed latents.\n",
    "    - $c_t \\in \\mathbb{R}^{H' \\times W' \\times C}$\n",
    "- A predictive function $\\phi$ predicts the future latents $\\hat{z}_{t+1}, \\hat{z}_{t+2}, ...$\n",
    "    - $\\hat{z}_{i} \\in \\mathbb{R}^{H' \\times W' \\times C}$\n",
    "- The following $c_{t+1}, ...$ and $\\hat{z}_{t+2}, ...$ are predicted autoregressively.\n",
    "- This is summarized in the following slide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DPC framework\n",
    "<img src=\"figs/dpc/dpc-fig-2.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DPC loss\n",
    "- $\\hat{z}_i \\in \\mathbb{R}^{H' \\times W' \\times C}$ is the predicted latent embedding at timestep $i$. Note that the spatial layout is kept.\n",
    "- $\\hat{z}_{i,k} \\in \\mathbb{R}^{C}$ is the predicted latent embedding at timestep $i$ and at spatial index $k$.\n",
    "- The DPC framework uses a variant of noise contrastive estimation in which the model is tasked to classify one positive sample against multiple negative samples. The samples to which to compare the predicted latents are taken from the $z_i$ computed by the encoder function $f$.\n",
    "- For the predicted embedding $\\hat{z}_{i,k}$ the only positive embedding is $z_{i,k}$.\n",
    "- For the predicted embedding $\\hat{z}_{i,k}$ the negative embeddings are taken from all $z_{j, m} \\ s.t. (i, k) \\neq (j, m)$.\n",
    "- This is illustrated in the previous slide.\n",
    "- The loss is then defined as $$\\mathcal{L} = -\\sum_{i,k} log \\frac{exp(\\hat{z}_{i,k}^T \\cdot z_{i,k})}{\\sum_{j,m} exp(\\hat{z}_{i,k}^T \\cdot z_{j,m})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DPC network architecture\n",
    "- The encoding function $f$ is implemented as a 3D resnet which uses a combinations of 2D and 3D convolution kernels.\n",
    "- The aggregation function $g$ is implemented as a ConvGRU with kernel size 1x1 thus sharing weights across spatial positions.\n",
    "- The authors mention that its preferable to use a weak aggregator in order to train a strong encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Experiments and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Discussion\n",
    "- Is this even right for what we want to capture?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
