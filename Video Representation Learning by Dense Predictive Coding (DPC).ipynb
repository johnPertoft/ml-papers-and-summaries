{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Video Representation Learning by Dense Predictive Coding (DPC)\n",
    "\n",
    "- Self-supervised technique for learning spatio-temporal representations of video.\n",
    "- For example, these features can be used in a downstream action recognition task.\n",
    "- Hopefully also suitable for capturing features that can be used in our pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n",
    "- Video data lends itself pretty well to self-supervised learning techniques and there's lots of it (youtube, etc).\n",
    "- Commonly self-supervised techniques in this domain are based on predicting future frames in a video.\n",
    "- The main difficulty in this lies in that the future is very much non deterministic.\n",
    "- If the goal is just to learn a representation for downstream tasks then it is likely unnecessary to waste model capacity on predicting pixel level data of future frames.\n",
    "- In this paper these future predictions are instead made in a latent representation space.\n",
    "- The DPC model is similar to techniques like contrastive predictive coding (CPC) and word2vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DPC overview\n",
    "- DPC is designed to predict the future latent representations based on the recent past.\n",
    "- DPC is trained using a variant of noise contrastive estimation which means that it never has to predict the exact future (i.e. not a regression loss) but rather just has to solve the multi-class classification problem of picking the correct future state out of many distractors.\n",
    "- To do this the model has to learn to capture the shared semantics of multiple future states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DPC framework\n",
    "- The input video is partitioned into multiple non-overlapping blocks $x_1, ..., x_n$ which each consists of a fixed number of frames.\n",
    "    - $x_i \\in \\mathbb{R}^{T \\times H \\times W \\times C}$\n",
    "- An encoding function $f$ encodes each block separately into the latent representations $z_1, ..., z_n$.\n",
    "    - $z_i \\in \\mathbb{R}^{H' \\times W' \\times C}$\n",
    "- An aggregation function $g$ computes a context representation $c_t = g(z_1, ..., z_t)$ given the observed latents.\n",
    "    - $c_i \\in \\mathbb{R}^{H' \\times W' \\times C}$\n",
    "- A predictive function $\\phi$ predicts the future latents $\\hat{z}_{t+1}, \\hat{z}_{t+2}, ...$\n",
    "    - $\\hat{z}_{i} \\in \\mathbb{R}^{H' \\times W' \\times C}$\n",
    "- The following $c_{t+1}, ...$ and $\\hat{z}_{t+2}, ...$ are predicted autoregressively.\n",
    "- This is summarized in the following slide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DPC framework\n",
    "<img src=\"figs/dpc/dpc-fig-2.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DPC loss\n",
    "- $\\hat{z}_i \\in \\mathbb{R}^{H' \\times W' \\times C}$ is the predicted latent embedding at timestep $i$. Note that the spatial layout is kept.\n",
    "- $\\hat{z}_{i,k} \\in \\mathbb{R}^{C}$ is the predicted latent embedding at timestep $i$ and at spatial index $k$.\n",
    "- The DPC framework uses a variant of noise contrastive estimation in which the model is tasked to classify one positive sample against multiple negative samples. The samples to which to compare the predicted latents are taken from the $z_i$ computed by the encoder function $f$.\n",
    "- For the predicted embedding $\\hat{z}_{i,k}$ the only positive embedding is $z_{i,k}$.\n",
    "- For the predicted embedding $\\hat{z}_{i,k}$ the negative embeddings are taken from all $z_{j, m} \\ s.t. (i, k) \\neq (j, m)$.\n",
    "- This is illustrated in the previous slide.\n",
    "- The loss should then encourage the model to produce a higher similarity (dot-product) for the positive pair than any of the negative pairs. They define the loss as follows. $$\\mathcal{L} = -\\sum_{i,k} log \\frac{exp(\\hat{z}_{i,k}^T \\cdot z_{i,k})}{\\sum_{j,m} exp(\\hat{z}_{i,k}^T \\cdot z_{j,m})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DPC curriculum learning\n",
    "- The negatives can be divided into groups of different difficulty.\n",
    "- Easy negatives from embeddings from other inputs in the batch.\n",
    "- Spatial negatives from embeddings from the same video but at different spatial indices.\n",
    "- Temporal negatives (hard negatives) from embeddings from the same video and the same spatial index but at different temporal indices.\n",
    "- The authors propose a curriculum learning strategy where the model is asked to progressively predict further into the future which both \n",
    "    - makes it progressively more difficult on its own\n",
    "    - and introduces more hard negatives during this process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DPC avoiding shortcuts\n",
    "- In order to learn useful representations, some efforts have to be made to restrict the model from learning more trivial shortcuts.\n",
    "- One of these is to try to disrupt simply learning optical flow which the authors combat by introducing framewise random data augmentation.\n",
    "- The authors say that the curriculum learning strategy also alleviates this problem.\n",
    "- Thirdly, the architecture design of splitting into blocks makes sure that the temporal receptive field does not allow \"cheating\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DPC network architecture\n",
    "- The encoding function $f$ is implemented as a 3D resnet which uses a combination of 2D and 3D convolution kernels.\n",
    "- The aggregation function $g$ is implemented as a ConvGRU with kernel size 1x1 thus sharing weights across spatial positions.\n",
    "- The predictor function $\\phi$ is implemented as a shallow MLP.\n",
    "- The authors mention that its preferable to use a weak aggregator in order to train a strong encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Experiments and analysis\n",
    "- They evaluate the model's ability to produce a useful representation by finetuning a classifier on three action classification datasets, UCF101, HMDB51, K400.\n",
    "- They study the following things\n",
    "    - An ablation study on the different design choices.\n",
    "    - The effect of training on larger and more diverse datasets.\n",
    "    - The correlation between performance in the self-supervised task and the downstream task.\n",
    "    - The effect on learnt representation when predicting further into the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Experiments and analysis: ablation study\n",
    "\n",
    "<img src=\"figs/dpc/dpc-table-1.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Experiments and analysis: benefits of large datasets\n",
    "\n",
    "<img src=\"figs/dpc/dpc-table-2.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Experiments and analysis: correlation of self-supervised and classification accuracy\n",
    "\n",
    "<img src=\"figs/dpc/dpc-fig-3.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Experiments and analysis: predicting further into the future\n",
    "\n",
    "<img src=\"figs/dpc/dpc-table-3.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Discussion\n",
    "- How difficult are these action classification datasets really? At least the UCF101 seems pretty to solve with much simpler models. But I could be mistaken.\n",
    "- What could be some other tasks to evaluate in? This would benefit our experimentation as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
