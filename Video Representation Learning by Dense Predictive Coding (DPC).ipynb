{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Video Representation Learning by Dense Predictive Coding (DPC)\n",
    "\n",
    "- Self-supervised technique for learning spatio-temporal representations of video.\n",
    "- For example, these features can be used in a downstream action recognition task.\n",
    "- Hopefully also suitable for capturing features that can be used in our pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n",
    "- Video data lends itself pretty well to self-supervised learning techniques.\n",
    "- There's lots of it (youtube, etc).\n",
    "- Commonly self-supervised techniques in this domain are based on predicting future frames in a video.\n",
    "- The main difficulty in this lies in that the future is very much non deterministic.\n",
    "- If the goal is just to learn a representation for downstream tasks then it is likely unnecessary to waste model capacity on predicting pixel level data of future frames.\n",
    "- In this paper these future predictions are instead made in a latent representation space.\n",
    "- The DPC model is similar to techniques like contrastive predictive coding (CPC) and word2vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DPC overview\n",
    "- DPC is designed to predict the future latent representations based on the recent past.\n",
    "- DPC is trained using a variant of noise contrastive estimation which means that it never has to predict the exact future but rather just has to solve the multi-class classification problem of picking the correct future state out of many distractors.\n",
    "- To do this the model has to learn to capture the shared semantics of multiple future states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DPC framework\n",
    "- The input video is partitioned into multiple non-overlapping blocks $x_1, ..., x_n$ which each consists of a fixed number of frames.\n",
    "    - $x_t \\in \\mathbb{R}^{T \\times H \\times W \\times C}$\n",
    "- Each block is then encoded into the latent representations $z_1, ..., z_n$ via an encoding function $f$.\n",
    "    - $z_t \\in \\mathbb{R}^{H' \\times W' \\times C}$\n",
    "- An aggregation function $g$ computes a context representation $c_t = g(z_1, ..., z_t)$ given the observed latents.\n",
    "    - $c_t \\in \\mathbb{R}^{H' \\times W' \\times C}$\n",
    "- A predictive function $\\phi$ predicts the future latents $\\hat{z}_{t+1}, \\hat{z}_{t+2}, ...$\n",
    "    - $\\hat{z}_{i} \\in \\mathbb{R}^{H' \\times W' \\times C}$\n",
    "- The following $c_{t+1}, ...$ and $\\hat{z}_{t+2}, ...$ are predicted autoregressively.\n",
    "- This is summarized in the following slide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DPC framework\n",
    "<img src=\"figs/dpc/dpc-fig-2.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DPC loss\n",
    "- $\\hat{z}_i \\in \\mathbb{R}^{H' \\times W' \\times C}$ is the predicted latent embedding at timestep $i$. Note that the spatial layout is kept.\n",
    "- $\\hat{z}_{i,k} \\in \\mathbb{R}^{C}$ is the predicted latent embedding at timestep $i$ and at spatial index $k$.\n",
    "- The DPC framework uses a variant of noise contrastive estimation in which the model is tasked to classify one positive sample against multiple negative samples. The samples to which to compare the predicted latents are taken from the $z_i$ computed by the encoder function $f$.\n",
    "- For the predicted embedding $\\hat{z}_{i,k}$ the only positive embedding is $z_{i,k}$.\n",
    "- For the predicted embedding $\\hat{z}_{i,k}$ the negative embeddings are taken from all $z_{j, m} \\ s.t. (i, k) \\neq (j, m)$.\n",
    "- This is illustrated in the previous slide.\n",
    "- The loss should then encourage the model to produce a higher similarity (dot-product) for the positive pair than any of the negative pairs. They define the loss as follows. $$\\mathcal{L} = -\\sum_{i,k} log \\frac{exp(\\hat{z}_{i,k}^T \\cdot z_{i,k})}{\\sum_{j,m} exp(\\hat{z}_{i,k}^T \\cdot z_{j,m})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DPC curriculum learning\n",
    "- The negatives can be divided into groups of different difficulty.\n",
    "- Easy negatives from embeddings from other inputs in the batch.\n",
    "- Spatial negatives from embeddings from the same video but at different spatial indices.\n",
    "- Temporal negatives (hard negatives) from embeddings from the same video and the same spatial index but at different temporal indices.\n",
    "- The authors propose a curriculum learning strategy where the model is asked to progressively predict further into the future which both \n",
    "    - makes it progressively more difficult on its own\n",
    "    - and introduces more hard negatives during this process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DPC avoiding shortcuts\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DPC network architecture\n",
    "- The encoding function $f$ is implemented as a 3D resnet which uses a combinations of 2D and 3D convolution kernels.\n",
    "- The aggregation function $g$ is implemented as a ConvGRU with kernel size 1x1 thus sharing weights across spatial positions.\n",
    "- The authors mention that its preferable to use a weak aggregator in order to train a strong encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Experiments and analysis\n",
    "- They study the following things\n",
    "    - An ablation study on the different design choices.\n",
    "    - The effect of training on larger and more diverse datasets.\n",
    "    - The correlation between performance in the self-supervised task and a downstream task.\n",
    "    - The effect on learnt representation when predicting further into the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Experiments and analysis: ablation study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Discussion\n",
    "- Is this even right for what we want to capture?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
