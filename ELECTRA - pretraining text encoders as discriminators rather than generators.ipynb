{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ELECTRA: pretraining text encoders as discriminators instead of generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "## Denoising objectives\n",
    "- Select a subset of the input tokens, mask them and ask the network to predict the original unmasked input.\n",
    "- Better than autoregressive language modeling task because of the possible bidirectionality.\n",
    "- Currently state of the art as pretraining objective in language representation learning.\n",
    "- **Problem:** They only learn from (only compute loss / gradients) from the subset of masked tokens which is usually around 15% of the tokens.\n",
    "\n",
    "## Replaced token detection\n",
    "- **Solution:** The authors propose a new training objective, *replaced token detection*.\n",
    "- In this task the network is asked to predict whether an input token is real or if it has been synhetically replaced, i.e. is a fake one.\n",
    "- This task allows the network to learn from all predictions which the authors show yield better sample efficiency and thus can train faster.\n",
    "- Another advantage is that this objective also achieves better performance when finetuned on downstream tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Replaced token detection pretraining task\n",
    "- An additional generator network is added whose task is to make predictions for masked out tokens just like in a standard masked language modeling (MLM) task (like BERT).\n",
    "- The output sequence (original tokens and predicted tokens) from the generator is the input to the discriminator which is the model that we're actually interested in.\n",
    "- The task of the discriminator is then to predict whether each token is real or if it has been synthetically replaced by the generator.\n",
    "- Both of these networks are trained on their respective tasks simultaneously.\n",
    "- Both networks are transformer based, but the generator is usually smaller than the discriminator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overview\n",
    "<img src=\"figs/electra/electra-fig-2.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Adversarial learning?\n",
    "- Superficially there are similarities to GAN training, but there are key differences.\n",
    "- If generator generated the actual input token it's not considered fake when computing the discriminator loss.\n",
    "- The generator is not trained to adversarially fool the discriminator which is the GAN setup but rather via standard maximum likelihood estimation.\n",
    "- There are challenges with an adversarial training setup in that gradients can't backpropagate through the sampling step of the generator.\n",
    "- The authors did an experiment where this challenge is circumvented using reinforcement learning but did not get good results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Experiments\n",
    "- After pretraining, only the discriminator is kept and the finetuned on the downstream tasks.\n",
    "- Finetune and evaluate on SQuAD and GLUE tasks by adding simple linear classifiers on top of ELECTRA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model extension: smaller generator\n",
    "- Firstly intended to reduce the amount of compute required per step\n",
    "- Also has an effect on performance. The authors speculate that this is because a larger generator makes the discriminator's task too difficult.\n",
    "\n",
    "<img src=\"figs/electra/electra-fig-3.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model extension: weight sharing\n",
    "- If generator and discriminator are same size all transformer layer weights can be shared, but there is a benefit in having a larger discriminator than generator in which case only token and position embeddings are shared.\n",
    "- In an experiment with a generator and discriminator of the same size, tying only token and position embeddings gave a performance increase on which tying all weights only improved very slightly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model extension: training algorithms\n",
    "- They experiment with a two-stage training procedure.\n",
    "- First train generator for $n$ steps.\n",
    "- Then initialize weights of discriminator with generator's weights (same size requirement).\n",
    "- Then train discriminator for $n$ steps with generator's weights frozen.\n",
    "- This procedure did not improve results. The authors speculate that the joint training provides a beneficial curriculum learning procedure.\n",
    "- As mentioned previously, adversarial learning via reinforcement learning was also attempted without success.\n",
    "\n",
    "<img src=\"figs/electra/electra-fig-3.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Small models results\n",
    "<img src=\"figs/electra/electra-table-1.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Large models results\n",
    "\n",
    "<img src=\"figs/electra/electra-table-3.png\"></img>\n",
    "\n",
    "<img src=\"figs/electra/electra-table-4.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Efficiency analysis\n",
    "- They run some experiments to get an understanding on where the gains are coming from.\n",
    "- **ELECTRA 15 %**, only compute discriminator loss on the 15% masked tokens.\n",
    "- **Replace MLM**, switch the [MASK] tokens with tokens from a generator (a second generator?).\n",
    "- **All-tokens MLM**, make predictions and compute losses for all tokens, not just the masked ones.\n",
    "<img src=\"figs/electra/electra-fig-4.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusion\n",
    "- New pretraining task for language representation learning proposed, *replaced token detection*.\n",
    "- The idea is to produce hard examples via a generator network.\n",
    "- The new task is more compute efficient and gives better performance on downstream tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
