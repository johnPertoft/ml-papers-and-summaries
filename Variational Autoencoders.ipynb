{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Variational Autoencoders\n",
    "This paper (which is actually called *Autoencoding Variational Bayes*) introduces a fairly general inference method for many directed graphical models with continuous latent variables. It is called *Stochastic Gradient Variational Bayes* (SGVB) and is based around approximating the evidence lower bound (ELBO) by using differentiable Monte Carlo estimated expectations which is different from normal variational bayes (variational inference) which just computes the iterative update steps from optimizing the ELBO.\n",
    "\n",
    "Since generative models are popular now they also apply this technique for generative purposes which results in the Variational Autoencoder (VAE). The basic generative model operates under the assumption that data is generated conditioned on some latent value $z$. In VAE we have a prior on $z$ which regularizes it to follow the given prior which means it will then be easier to input new $z$ that generates reasonable $x$.\n",
    "\n",
    "\n",
    "## The problem\n",
    "Have i.i.d. discrete or continuous data $X$ that is assumed to come from a generative process where\n",
    "1. a latent variable $z^{(i)}$ is drawn from a prior $p_\\theta(z)$\n",
    "2. an observation $x^{(i)}$ is generated from a conditional distribution $p_\\theta(x\\ |\\ z^{(i)})$.\n",
    "\n",
    "The goal is then often to approximate the posterior (the true posterior often being intractable to compute). Sometimes the standard mean field variational inference can be enough, but sometimes any reasonable mean field VI can also lead to inractable integrals. This can appear in many cases of moderately complicated likelihood functions, like a neural network with a non linear hidden layer for example. \n",
    "\n",
    "This is the main problem that they want to solve with the following proposed inference method. Also important for large datasets where minibatches is the only possibility to get updates from. Training on streaming data is also useful which this model is capable of.\n",
    "\n",
    "## The solution\n",
    "They introduce the recognition model $q_\\phi(z\\ |\\ x)$ which is an approximation of the true posterior $p_\\theta(z\\ |\\ x)$. This can also be seen as a probabilistic encoder that given an $x$ gives a latent code $z$. The generative part of the model $p_\\theta(x\\ |\\ z)$ can be seen as probabilistic decoder that given a latent code $z$ gives a distribution over possible $x$.\n",
    "\n",
    "This solution does not make the same assumption as mean field variational inference where $q$ is factorized over the latent variables and variational parameters $\\phi$ are computed from a closed from expectation expression.\n",
    "\n",
    "The following method then learns the recognition model parameters (variational parameters) $\\phi$ and the generative model's parameter $\\theta$ jointly.\n",
    "\n",
    "**Variational bound:** From basic variational inference we know\n",
    "\n",
    "\\begin{align*}\n",
    "log\\ p(x^{(i)}) &= KL(q_\\phi(z\\ |\\ x^{(i)})\\ ||\\ p_\\theta(z\\ |\\ x^{(i)})) + \\mathcal{L}(\\theta, \\phi, x^{(i)}) && \\text{KL is non negative} \\\\\n",
    "\\\\\n",
    "log\\ p(x^{(i)}) &\\geq \\mathcal{L}(\\theta, \\phi, x^{(i)}) = \\mathbb{E}_{q_\\phi(z\\ |\\ x)} \\left[ log\\ p_\\theta(x,z) - log\\ q_\\phi(z\\ |\\ x) \\right] && \\text{Variational lower bound} \\\\\n",
    "\\\\\n",
    "\\mathcal{L}(\\theta, \\phi, x^{(i)}) &= -KL(q_\\phi(z\\ |\\ x^{(i)})\\ ||\\ p_\\theta(z)) + \\mathbb{E}_{q_\\phi(z\\ |\\ x^{(i)})} \\left[ log\\ p_\\theta(x^{(i)}\\ |\\ z) \\right] && \\text{TODO: check this}\n",
    "\\end{align*}\n",
    "\n",
    "The idea is then to use a **monte carlo estimator** for this variational bound.\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{L}(\\theta, \\phi, x^{(i)}) &= \\mathbb{E}_{q_\\phi(z\\ |\\ x)} \\left[ log\\ p_\\theta(x,z) - log\\ q_\\phi(z\\ |\\ x) \\right] \\\\\n",
    "&\\approx \\frac{1}{L} \\sum^L_{l=1} log\\ p_\\theta(x,z) - log\\ q_\\phi(z\\ |\\ x) \\\\\n",
    "z_l &\\sim q_\\phi(z\\ |\\ x) && \\text{samples}\n",
    "\\end{align*}\n",
    "\n",
    "Then we would like to differentiate this with respect to $\\theta$ and $\\phi$ to optimize but this would exhibit very high variance (according to an earlier paper), and also not possible to backprop through a sampling operation (?). Instead they use the **reparametrization trick** which is a way to construct samples $z \\sim q_\\phi(z\\ |\\ x)$ deterministically from a differentiable transformation $g_\\phi$ using another random variable.\n",
    "\n",
    "\\begin{align*}\n",
    "\\epsilon &\\sim p(\\epsilon) && \\text{random seed independent of $\\phi$} \\\\\n",
    "z &= g_\\phi(\\epsilon, x) && \\text{differentiable perturbation}\n",
    "\\end{align*}\n",
    "\n",
    "Using all this, we can now form monte carlo estimates of the expectation from the lower bound, which we then differentiate to get update steps for $\\theta$ and $\\phi$. This is the generic **Stochastic Gradient Variational Bayes estimator** (SGVB).\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{q_\\phi(z\\ |\\ x^{(i)})} \\left[ log\\ p_\\theta(x,z) - log\\ q_\\phi(z\\ |\\ x) \\right] &\\approx \\frac{1}{L} \\sum^L_{l=1} log\\ p_\\theta(x^{(i)}, z^{i, l}) - log\\ q_\\phi(z^{i, l}\\ |\\ x) \\\\\n",
    "z^{(i, l)} &= g_\\phi(\\epsilon^{(i, l)}, x^{(i)}) \\\\\n",
    "\\epsilon^{(i, l)} &\\sim p(\\epsilon)\n",
    "\\end{align*}\n",
    "\n",
    "So in practice, we just sample some $\\epsilon$ for each $x^{(i)}$, use both of them to sample $z^{(i, l)}$ from approximate posterior through $g_\\phi$. The $z^{(i, l)}$ are then put through the reconstruction part $p(x\\ |\\ z)$.\n",
    "\n",
    "All in all, this gives the following **training algorithm**\n",
    "```\n",
    "theta, phi = init_parameters()\n",
    "while not done:\n",
    "    X_b = next_minibatch()\n",
    "    epsilon = sample_noise_inputs()\n",
    "    g_theta, g_phi = compute_gradients_of_estimator(theta, phi, X_b, epsilon)\n",
    "    theta, phi = update_parameters(g_theta, g_phi) /* sgd or other gradient optimizer */\n",
    "    \n",
    "    \n",
    "```\n",
    "\n",
    "The authors point out that the KL divergence of the lower bound can often be analytically integrated leaving only the expected reconstruction error $\\mathbb{E}_{q_\\phi(z\\ |\\ x^{(i)})} \\left[ log\\ p_\\theta(x^{(i)}\\ |\\ z) \\right]$ that needs to be monte carlo sample estimated. \n",
    "\n",
    "The KL divergence term can then be interpreted as regularizing $\\phi$ to encourage the approximate posterior $q_\\phi(z\\ |\\ x)$ to be close to the prior $p_\\theta(z)$.\n",
    "\n",
    "### Notes on reparametrization\n",
    "Basically: Rewrite a continuous (does it have to be?) random variable from some complex distribution as a function together with another random variable instead.\n",
    "\n",
    "Useful since we can rewrite an expectation wrt the complicated distribution as an expectation wrt the simple new introduced variable instead such that the monte carlo estimate of it is differentiable wrt the parameters we are interested in (the encoding network in the case of VAE).\n",
    "\n",
    "To choose a reparametrization of $q_\\phi(z\\ |\\ x)$ three approaches are\n",
    "1. A \"location-scale\" type distribution where $\\epsilon$ distribution has location = 0, scale = 1. Then $g_\\phi = location + scale * \\epsilon$ where location and scale are outputs from the recognition model. E.g. gaussian, uniform, etc\n",
    "2. Tractable inverse CDF, $\\epsilon$ is uniform (0, 1), $g_\\phi$ is the inverse cdf\n",
    "3. Composition, by expressing the random variables as different transformations\n",
    "\n",
    "TODO: Don't fully understand 2 and 3 here, what is the intuition?\n",
    "\n",
    "## Variational Autoencoder\n",
    "The VAE is a special but useful case where the probabilistic encoder $q_\\phi(z\\ |\\ x)$ and decoder $p_\\theta(x\\ |\\ z)$ are based on neural networks. \n",
    "\n",
    "Here they pick the prior $p_\\theta(z) = \\mathcal{N}(z; \\mathbf{0}, \\mathbf{I})$ (i.e. no parameters in prior in this case). $p_\\theta(x\\ |\\ z)$ can be a gaussian or bernoulli (TODO other examples? is it about conjugate distributions) depending on type of data being modeled.\n",
    "\n",
    "The decoding MLP outputs the parameters of the output distribution. The true posterior is intractable but they let the encoding distribution be represented by an MLP which outputs the parameters of the approximate posterior $q_\\phi(z\\ |\\ x)$. They assume an approximate diagonal covariance.\n",
    "\n",
    "\\begin{align*}\n",
    "log\\ q_\\phi(z\\ |\\ x^{(i)}) &= log\\ \\mathcal{N}(z; \\mu^{(i)}, \\sigma^{2(i)}\\mathbf{I}) \\\\\n",
    "\\\\\n",
    "\\mu^{(i)},\\ \\sigma^{2(i)} & \\quad \\text{outputs of encoding MLP}\n",
    "\\end{align*}\n",
    "\n",
    "Samples from approximate posterior are obtained by using the reparametrization trick in the following way\n",
    "\n",
    "\\begin{align*}\n",
    "z^{(i, l)} &= g_\\phi(x^{(i)}, \\epsilon^{(l)} = \\mu^{(i)} + \\sigma^{(i)} \\odot \\epsilon^{(l)} \\\\\n",
    "\\\\\n",
    "\\epsilon^{(l)} &\\sim p(\\epsilon) = \\mathcal{N}(\\mathbf{0}, \\mathbf{I})\n",
    "\\end{align*}\n",
    "\n",
    "In this case, since both the prior and the approximate posterior are gaussian, the KL divergence part of the lower bound estimator can be analytically integrated and differentiated which means that the following estimator can be used.\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{L}(\\theta, \\phi, x^{(i)}) &\\approx \\frac{1}{2} \\sum^D_{d=1} \\left( 1 + log \\left( (\\sigma_d^{(i)})^2 \\right) - (\\mu_d^{(i)})^2 - (\\sigma_d^{(i)})^2 \\right) + \\frac{1}{L} \\sum^L_{l=1} log\\ p_\\theta(x^{(i)}\\ |\\ z^{(i, l)})\n",
    "\\end{align*}\n",
    "\n",
    "### Example applications\n",
    "In the paper they use a VAE to generate samples close to the mnist and frey face datasets.\n",
    "\n",
    "<img src=\"figs/vae/vae-mnist-frey.png\" width=\"65%\" height=\"65%\">\n",
    "\n",
    "Other things it could be used for \n",
    "\n",
    "* **Learning representations**\n",
    "* **Compression?**\n",
    "* **Can probably have deeper models too**\n",
    "* **Using other output distributions**\n",
    "\n",
    "## Discussion and thoughts\n",
    "The main thing here is the inference method which can be applied to many directed graphical models with continuous latent variables. \n",
    "\n",
    "A nice thing with this method is that it is a way to not let the number of parameters increase with the size of the data which is usually the case with variational inference (?). This is simply because we have the encoder network's parameters that are shared for all inputs to compute the approximate distribution over the latent variables.\n",
    "TODO: Source for this\n",
    "\n",
    "\"\n",
    "The neural network used in the encoder (variational distribution) does not lead to any richer approximating distribution.  It is a way to amortize inference such that the number of parameters does not grow with the size of the data (an incredible feat, but not one for expressivity!) - Dustin tran's blog \n",
    "The inference network takes data as input and outputs the local variational parameters relevant to each data point. The optimal inference network outputs the set of Gaussian parameters which maximizes the variational objective. This means a variational auto-encoder with a perfect inference network can only do as well as a fully factorized Gaussian with no inference network.\n",
    "\"\n",
    "\n",
    "As a generative model comparing VAE with Generative Adversarial Networks (GAN) is interesting because there are many similar papers where in one they use VAE and in the corresponding one they use GAN. \n",
    "\n",
    "* At least on image data, GANs seem to generate more defined because of the different training objective\n",
    "* No control of latent space in standard GAN\n",
    "* \n",
    "\n",
    "\n",
    "There are also attempts where they are combined as in TODO which \n",
    "\n",
    "VAEs better at compression? better for representation learning? how to use GAN for this though?\n",
    "\n",
    "Standard GAN - no control of latent space?\n",
    "\n",
    "GAN jensen shannon divergence? write about it in gan review?\n",
    "\n",
    "Can we use supervision in anyway to be able to control the latent space further? Like in adversarial autoencoders.\n",
    "\n",
    "TODO:  Hmm, did I misunderstand something? when generating new, do you sample from prior or approximative posterior? It should be ok to sample from prior. But I guess we could also sample from the posterior of an existing input to get something similar?\n",
    "\n",
    "### Problems with VAE\n",
    "Blurry images because of \n",
    "\n",
    "In the case of image data\n",
    "\n",
    "Generative Adversarial Networks (GAN) usually give more defined images because of the different training objective. There have been some work where VAE and GANs have been combined such that the recontruction loss of the VAE is replaced by a loss coming from a discriminatory network. The discriminatory network in this case tries to distinguish generated samples from the VAE from the corresponding real sample that was an input to the VAE.\n",
    "\n",
    "\n",
    "## TODO\n",
    "\n",
    "Check the appendix as well, where they do the fully variational inferring of posterior over the parameters\n",
    "\n",
    "read https://arxiv.org/abs/1406.5298 semisupervised vae\n",
    "read https://arxiv.org/abs/1605.06197 stick breaking vae (complex prior)\n",
    "\n",
    "actually few similarities with normal autoencoders, just resembles them\n",
    "\n",
    "ancestral sampling?\n",
    "\n",
    "We assume that the prior pθ\n",
    "∗ (z) and likelihood pθ\n",
    "∗ (x|z) come from\n",
    "parametric families of distributions pθ(z) and pθ(x|z), and that their PDFs are differentiable almost\n",
    "everywhere w.r.t. both θ and z.\n",
    "\n",
    "Very importantly, we do not make the common simplifying assumptions about the marginal or posterior\n",
    "probabilities. Conversely, we are here interested in a general algorithm that even works effi-\n",
    "ciently in the case of: intractability and large datasets\n",
    "\n",
    "when is it useful\n",
    "\n",
    "SGVB is distinguished from classical variational Bayes by it’s use of differentiable\n",
    "Monte Carlo (MC) expectations.\n",
    "\n",
    "https://openreview.net/pdf?id=S1jmAotxg nice for some other insights about vae\n",
    "\n",
    ", SGVB allows for a broad class of non-conjugate approximate posteriors and thus has the\n",
    "potential to expand Bayesian nonparametric models beyond the exponential family distributions to\n",
    "which they are usually confined.\n",
    "\n",
    "discussion, why high variance with naive mc estimator?\n",
    "\n",
    "can show something asdad about normal autoencoders, but this is not enough to learn a useful representation\n",
    "\n",
    "robustness to high dimensional latent space, ie will not overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
