{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Is All You Need\n",
    "* This paper deals with sequence transduction (e.g. neural language translation models).\n",
    "* Most of these models have been based on recurrent or convolutional architectures.\n",
    "* This paper does not use recurrent or convolutional architectures and propose a new architecture that they call *Transformer*.\n",
    "* A bit of a paradigm shift!\n",
    "* On English to German they reach BLEU score of 28.4\n",
    "* On English to French 41.0 BLEU score\n",
    "\n",
    "## Previous State of the Art\n",
    "* *Encoder-Decoder* structure\n",
    "    * One encoder that takes the source sequence and produces something that is fed into a decoder which then produces the output sequence.\n",
    "    * Both recurrent and convolutional (and combinations) have been used for both encoder and decoder (I think).\n",
    "    * For recurrent models (usually bidirectional LSTMs)\n",
    "        * The encoder sequentially processes the source sequence $x = (x_1, ..., x_n)$ to produce hidden state representations $h = (h_1, ..., h_n)$. Each $h_t$ is computed as a function of the token (word vector) at time step $t$ and the previous hidden state $h_{t-1}$\n",
    "        * $h$ is given to the decoder which then sequentially produces an output sequence $y = (y_1, ..., y_m)$\n",
    "            * Sometimes only $h_n$ is used as input to the decoder?\n",
    "            * At each timestep the decoder also outputs a hidden state which is fed into the next invocation of it.\n",
    "            * The decoder is auto-regressive, meaning the produced tokens at previous timesteps are also fed into the decoder when producing the next.\n",
    "\n",
    "## Difficulties\n",
    "* Recurrent models are because of their sequential computation not as parallelizable within training examples.\n",
    "    * Slower\n",
    "    * More memory consumption leading to smaller batch sizes.\n",
    "* *Long range dependencies*\n",
    "    * With RNN based architectures, all memory has to be stored in a fixed sized vector that is passed a long through time. These networks are usually trained to keep important information and discard unnecessary information which can often yield worse translation when dependencies are far apart.\n",
    "    * When the decoder predicts a token, the information needed to do this well must be present in the hidden state fed in. If the path between this hidden state and the hidden state for the source tokens that matter for this prediction are far apart in terms of hidden state computation steps this becomes hard.\n",
    "    * The goal is to make these paths shorter so that information needed is easily accessible at any position and one way to do this is *attention* explained below.\n",
    "\n",
    "## Attention\n",
    "* The goal of attention is to reduce the distance of information flow so that the model can get the relevant information more easily.\n",
    "* Self-attention is to relate different positions of a single sequence to each other. Basically, pass a weighted average of all previous (and future states dep. on architecture?) states to the computation for a token.\n",
    "* Attention functions can be summarized as a mapping between a query and a set of key-value pairs to an output.\n",
    "    * The output is a weighted sum of the values\n",
    "    * Weights are computed by a compatibility function of query and keys.\n",
    "    * Basically the key-values should store interesting stuff and the queries are questions about this.\n",
    "* In self-attention keys, values and queries are all the sequence itself?\n",
    "    * Sort of: How does a token in a sequence relate to the other tokens in the sequence.\n",
    "\n",
    "## This Paper, the Transformer model\n",
    "* No recurrence/convolutions, instead just relying on attention.\n",
    "* Still have the encoder-decoder structure.\n",
    "    * The entire source sequence is fed to the encoder.\n",
    "    * The output of the encoder is input length $n$ times $d_{model}$\n",
    "    * The encoder output and the tokens produced so far are fed into the decoder which produces probabilities for the next token to predict. This is repeated to do a translation.\n",
    "    * For training, this means that we can compute gradients and update weights for each single token in a sequence to sequence pair. Compare to an RNN where backpropagation through each timestep would happen.\n",
    "\n",
    "### Encoder\n",
    "* Source tokens into embeddings\n",
    "* Add positional info\n",
    "* $N = 6$ identical layers\n",
    "    * First: multi-head attention with residual connection and then layer normalization\n",
    "    * Second: position-wise feed forward network with residual connection and then layer normalization\n",
    "    * Both (and word embeddings) produce outputs with dimension $d_{model} = 512$ \n",
    "\n",
    "### Decoder\n",
    "* Output tokens into embeddings\n",
    "* Add positional info\n",
    "* $N = 6$ identical layers\n",
    "    * First: masked multi-head attention over output embeddings with residual connection and then layer normalization\n",
    "        * The mask is to ensure that predictions for position $i$ can only depend on previous outputs of the decoder (since the others haven't been predicted yet).\n",
    "    * Second: multi-head attention over encoder's output with queries from previous sublayer with residual connection and then layer normalization\n",
    "    * Third: position-wise feed forward network with residual connection and then layer normalization\n",
    "* Linear + softmax to probabilities of next token\n",
    "\n",
    "<img src=\"figs/attentionisallyouneed/model.png\" width=\"40%\">\n",
    "\n",
    "\n",
    "### Common\n",
    "\n",
    "#### Positional info encoding\n",
    "* Since no convolution or recurrence, they also feed in positional information\n",
    "* Added to token embedding, so also $d_{model}$ dimension\n",
    "* Can be learned or fixed\n",
    "* They use different sinusoids for each dimension with token position as input\n",
    "\n",
    "#### Position-wise feed forward networks\n",
    "* 1 hidden layer neural network with relu at hidden layer.\n",
    "* It's applied at each position separately and identically.\n",
    "* Weights are shared for different positions but not shared across the $N$ layers.\n",
    "\n",
    "### Attention\n",
    "    \n",
    "#### Scaled Dot-Product Attention\n",
    "* Input is queries and keys.\n",
    "* Compute dot product between queries and keys -> similarity.\n",
    "* Apply softmax on dot products.\n",
    "* Use the softmax output as weights on the values to compute the output.\n",
    "\n",
    "#### Multi-Head Attention\n",
    "* Instead of just one attention function they do it multiple time as follows.\n",
    "* They project the queries, keys, and values $h$ times with different linear projections.\n",
    "* On each projected version of queries, key, and values the scaled dot product attention is applied in parallel.\n",
    "* All outputs are concatenated and linearly projected to final output.\n",
    "\n",
    "#### How it's applied\n",
    "* In the second attention sublayer of the decoder\n",
    "    * Keys and values come from the encoder's output.\n",
    "    * Queries come from the previous self-attention layer of the decoder.\n",
    "* In the self-attention sublayers of the encoder\n",
    "    * The keys, values, and queries come from the previous layer (not sublayer) of the encoder.\n",
    "    * Each position can attend to all other positions in the sequence as represented by the previous layer.\n",
    "* In the self-attention sublayers of the decoder\n",
    "    * The keys, values, and queries come from the previous layer (not sublayer) of the decoder.\n",
    "    * Each position can attend to all previous (timewise) positions in the produced sequence by the previous layer.\n",
    "* TODO: Still not sure exactly how the queries, keys, values are defined from the output.\n",
    "\n",
    "### Training\n",
    "* Adam optimizer\n",
    "* First linearly increasing learning rate, then decreasing proportionally to inverse sqrt of steps.\n",
    "* Dropout at each sublayer before adding residual and layer normalizaton.\n",
    "* Dropout at sum of token and position embeddings.\n",
    "* Label smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
