{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICLR 2019 notes\n",
    "\n",
    "## TLDR\n",
    "* Lots of reinforcement learning, didn't focus so much on this.\n",
    "* Some other keywords (that felt thematic to this conference)\n",
    "    * Robustness\n",
    "    * Transfer learning\n",
    "    * Unsupervised learning\n",
    "    * Meta-learning\n",
    "* Some of the posters can be found here https://postersession.ai/\n",
    "\n",
    "\n",
    "## Day 1\n",
    "\n",
    "### Invited talk: Recent developments in fairness (Cynthia Dwork)\n",
    "* Different definitions of fairness.\n",
    "* Learning fair representations where sensitive feature has been removed. (Adversarial fairness).\n",
    "\n",
    "### Invited talk: Learning representations using causal invariance (Leon Bottou)\n",
    "* Learning algorithms often capture correlations we are not actually interested in.\n",
    "* Suppose we have datasets each showcasing the same concept but under different biases (environments).\n",
    "* We want to learn what is common between them and ignore the spurious correlations.\n",
    "* We can do this by projecting to a representation that has a causal invariance criterion.\n",
    "* Shuffling data could be a loss of information.\n",
    "    * Lose information about what has changed and what remains under different circumstances (environments).\n",
    "\n",
    "### BigGAN\n",
    "* Conditional image generation.\n",
    "* Self-attention GAN is the base model they improve on.\n",
    "* Tricks from other papers\n",
    "    * Spectral normalization in G and D.\n",
    "    * Imbalanced learning rates and number of train steps for G and D.\n",
    "    * Hinge GAN loss.\n",
    "    * Class conditional batch norm.\n",
    "    * Shared batch norm statistics across devices.\n",
    "    * Orthogonal initialization.\n",
    "    * Exponential moving average of weights to produce final model.\n",
    "* Innovations\n",
    "    * More parameters are beneficial (scaling up).\n",
    "    * Larger batch size is beneficial.\n",
    "    * Ortogonal regularization.\n",
    "    * Shared embedding space with linear projection at each resblock.\n",
    "    * Hierarchical latent space.\n",
    "    * Truncation trick, resampling values with too high magnitudes gives tradeoff capability between quality and variation.\n",
    "* Insights\n",
    "    * Guaranteeing training stability comes with a quality cost (with this approach at least).\n",
    "    * Instead it seems better (at least for now) to let it train with unstable hparams and then just take the last checkpoint before training goes bad.\n",
    "* Good paper for overview of many modern GAN tricks.\n",
    "\n",
    "### The Lottery Ticket Hypothesis\n",
    "* It's often possible to prune trained networks to make them smaller in size and faster to do inference with.\n",
    "* However, it's also difficult to trained the pruned network architecture from scratch.\n",
    "* The lottery ticket hypothesis states that the big unpruned nets contain sub-networks that got the \"winning ticket\", i.e. got a good initialization that would allow it to be trained in isolation and still reach comparable performance to the full network.\n",
    "* This paper presents an algorithm to find winning tickets.\n",
    "    * But only after training has converged?\n",
    "    * Does the winning ticket work well if we change to another (similar) dataset? E.g. another marketplace.\n",
    "* Question: when training the full network the weight of a sub-network would still be touch by gradients from other weights. How does this influence the whole thing?\n",
    "\n",
    "### Workshop: Deep generative models for Structured Data\n",
    "\n",
    "#### Continuous-Output Language Generation (Yulia Tsvetkov)\n",
    "* A softmax layer is often used as output layer for language generation models.\n",
    "* It's very computationally heavy for large vocabulary sizes (and high memory usage).\n",
    "* Often limiting the vocabulary size.\n",
    "* GANs for text have not worked well due to non continuous output and getting gradients back.\n",
    "* The suggestion here is to instead output a continuous embedding which alleviates the computational complexity and is an approach for text GANs.\n",
    "* Questions: Only greeding decoding right now, but they were working on something else.\n",
    "\n",
    "\n",
    "## Day 2\n",
    "\n",
    "### Invited talk: Adversarial Machine Learning (Ian Goodfellow)\n",
    "* Overview talk on adversarial machine learning.\n",
    "* He went through different areas within machine learning where adversarial machine learning is used.\n",
    "    * Generative modeling.\n",
    "    * Security (adversarial examples).\n",
    "    * Fairness.\n",
    "    * Domain adaptation.\n",
    "    * Label efficiency.\n",
    "    * etc.\n",
    "\n",
    "### Learning Robust Representations by Projecting out Superficial Statistics\n",
    "TODO\n",
    "\n",
    "### Poster session 1\n",
    "\n",
    "#### Switchable Normalization\n",
    "* They introduce a new type of normalization layer, Switch Norm (SN), that learns importance weights of BN, IN, and LN.\n",
    "* At each layer, separate statistics are collected according to BN, IN, LN. These statistics are then weight-averaged and used in the normalization, i.e. subtract the weighted mean and divide by the weighted stddevs.\n",
    "* They see that SN prefers BN for backbone networks and LN in layers close to the head of object detection models.\n",
    "* They see that it picks IN for style transfer models (which is what they use I think), so it seems to make good choices.\n",
    "* Seems nice to not have to experiment with different normalization layers.\n",
    "* Robust to varying batch sizes and works well even for very small batch sizes.\n",
    "* Doesn't have sensitive hyper params like GN.\n",
    "\n",
    "#### The Singular Values of Convolutional Layers\n",
    "* Singular values are bad because they lead to exploding or vanishing gradients.\n",
    "* Operator norm means to constrain the maximum singular value.\n",
    "* The effect of this is that linear transformations (like conv layers) can't make too big changes (Lipschitz constant).\n",
    "* Regularizing the operator norm can lead to better generalization and robustness.\n",
    "* Previous work has also identified this as a problem but only used approximations to compute the singular values for conv layers (spectral norm?).\n",
    "* Operator norm is complementary to batch norm.\n",
    "\n",
    "#### Approximating CNNs with bag of local features\n",
    "* Aka BagNet\n",
    "* CNNs is applied on small image patches of the full image which outputs logits.\n",
    "* Many patches are evaluated like this which yields a heatmap per class. \n",
    "* The heatmaps are summed to produce the \"votes\" per class which is then fed into a softmax for predictions.\n",
    "    * Maybe the dog class was activated in many local patches, which would give it a lot of \"votes\".\n",
    "* Size of patches?\n",
    "* This had a connection to the texture bias that CNNs were shown to have.\n",
    "    * Would probably fail for the style transfered dataset (where texture was altered) in that paper.\n",
    "\n",
    "### Poster session 2\n",
    "Lots of papers about GANs and adversarial examples. Is adversarial examples something we need to consider at Schibsted?\n",
    "\n",
    "#### Fixup Initialization\n",
    "* Normalization in deep learning is often credited for \n",
    "    * training stabilization\n",
    "    * enabling higher learning rate\n",
    "    * accelarate convergence\n",
    "    * increase generalization\n",
    "* The reasons for these effects have not been proven yet and authors show that they are not unique to normalization and suggest this alternative method.\n",
    "* An alternative to normalization for residual networks called *fixup* or *fixed-update* initialization.\n",
    "* Cool because saving on memory usage.\n",
    "* The steps to convert a resnet with normalization to fixup initialization instead:\n",
    "    * Remove the normalization layers.\n",
    "    * For each residual branch:\n",
    "        * Initialize one weight layer (conv) to zeros.\n",
    "        * Initialize the other ones with some standard initiation and then rescale the weights by $L^{-\\frac{1}{2m - 2}}$\n",
    "        * Add a scalar multiplier initialized to 1.\n",
    "        * Add a scalar bias initialized to 0 before each weight and before each activation layer.\n",
    "* Experiments in image classification (resnet variants) and machine translation (transformers).\n",
    "* Should be useful for small batch sizes? How does it compare to normalization techniques designed for smaller batch sizes (e.g. layer norm)?\n",
    "* How does it relate to self-normalizing networks (SELU)?\n",
    "\n",
    "\n",
    "## Day 3\n",
    "\n",
    "### Invited talk: Learning Deep Representations by Mutual Information Estimation and Maximization (Devon Hjelm)\n",
    "* Unsupervised learning of image representations.\n",
    "* Key is to maximize the mutual information (MI) between an image and the encoder's computed representation of it.\n",
    "* MI is hard to compute, but recent advances are leveraged, via neural network.\n",
    "* MI with full input doesn't always work so well, rather MI with local regions of the input works better.\n",
    "* MI is combined with prior matching (like adversarial autoencoder) to get desired statistical properties of representation.\n",
    "* Sort of a triplet learning thing, maximize mutual information between $X_{image}$ and representation $y_{image}$ while minimizing mutual information between $X_{other}$ and $y_{image}$.\n",
    "\n",
    "### Poster session 1\n",
    "\n",
    "### Poster session 2\n",
    "\n",
    "#### Towards Understanding Regularization in Batch Normalization\n",
    "* Batchnorm improves both convergence speed and generalization.\n",
    "* TODO\n",
    "\n",
    "#### Decoupled weight decay\n",
    "* Paper from previous paper reading session.\n",
    "* l2 regularization plus adam often gives bad results.\n",
    "* Don't let adam make its updates based on a l2 regularized loss.\n",
    "* Instead update adam based on normal loss only and then do the weight decay as a separate step.\n",
    "\n",
    "#### Deep Anomaly Detection with Outlier Exposure\n",
    "TODO\n",
    "\n",
    "## Day 4\n",
    "\n",
    "### Pay Less Attention with Lightweight and Dynamic Convolutions\n",
    "* The authors ask themselves whether self-attention (like in transformers) are required to get good performance and whether a more limited context is actually enough for many NLP tasks.\n",
    "* More limited context (as with CNNs) is interesting because it would be faster.\n",
    "* Standard CNNs have fixed weights over time which is less than ideal compared to self-attention and RNNs.\n",
    "* They introduce dynamic convolutions to address this.\n",
    "* Dynamic convolutions take the current word embedding and computes a kernel to apply to the neighborhood in order to compute the next representation layer.\n",
    "* A challenge lies in the fact that a lot of parameters must be predicted (~100M per layer). This is handled by first considering depthwise convolutions (which is a lot fewer but still not enough? they do experiments with both) and secondly *lightweight convolutions* where some weights (heads) are shared.\n",
    "* They modify a transformer to use dynamic convolutions (n x lightweight convolutions) and get slightly better accuracy and also faster in some translation tasks.\n",
    "* Also really useful in text summarization in their experiments.\n",
    "* They also do experiments with only lightweight convolutions, I guess same kernel for all time steps but still computed based on something?\n",
    "\n",
    "### Smoothing the Geometry of Probabilistic Box Embeddings\n",
    "* Vector embeddings have some problems.\n",
    "    * Can't capture regions (mammal region should cover all mammal species).\n",
    "    * Can't capture asymmetry (rabbit is a mammal, but mammal is not a rabbit). At least not with similarity measures like dot product.\n",
    "* One attempt to address this is gaussian representations which also has disjointness (can avoid region overlap of two concepts) but it's not closed under intersection (intersection of two gaussian are not necessarily a gaussian).\n",
    "* Another representation is via cones (what does this mean? a box that extends to infinity?) which has the closed under intersection property but not the disjointness property.\n",
    "* Box representation solves both of these and has all four properties.\n",
    "* The representations are initialized randomly.\n",
    "* When training this there are difficulties with lack of gradients when a box is outside another box that it should be inside because the probability is zero.\n",
    "\n",
    "### Ordered Neurons: Integrating Tree Structures into RNNs (best paper award)\n",
    "TODO\n",
    "\n",
    "### Poster session 1\n",
    "\n",
    "#### Poincare Glove: Hyperbolic Word Embeddings\n",
    "TODO\n",
    "\n",
    "#### Hyperbolic Attention Networks\n",
    "TODO\n",
    "\n",
    "#### Universal Transformers\n",
    "* RNNs are slow to train, can't benefit from parallelization. \n",
    "* RNNs are difficult to train due to long range dependencies.\n",
    "* Vanilla transformers was a solution to this in some tasks.\n",
    "* But vanilla transformers are bad in other tasks, e.g. would fail to generalize in simple tasks like copying strings.\n",
    "* Universal Transformers (UT) are introduced to solve these weaknesses of transformers.\n",
    "* UTs are still parallel in time, i.e. each time step can access/attend to representations from all other time steps.\n",
    "* UTs instead have recurrent depth coupled with dynamic halting of recurrence per time step.\n",
    "* They got sota on both some machine translation task and the type of tasks in which vanilla transformers would fail.\n",
    "* Author said it should work well as a drop in replacement for vanilla transformers.\n",
    "\n",
    "#### Structured Neural Summarization\n",
    "* Previously most standard summarization models are based on some sequence-to-sequence model.\n",
    "* Here they add a way to exploit known relationships between elements in the input by combining sequence encoders with graph neural networks (GNN).\n",
    "* Input is a sequence of tokens which is fed through a sequence encoder to get representations for each token.\n",
    "* These representations are fed into the GNN as initial node representations which computes updated states that can then be fed to a decoder.\n",
    "* The GNN also takes the binary relationships between nodes as input.\n",
    "* These relationsships can be\n",
    "    * Inferred sentence structure relationships (these statistical models are accurate enough).\n",
    "    * Relationships to describe which node follows which node in the sequence.\n",
    "    * Relationships describing if a token represents a person, or similar knowledge graph relations.\n",
    "    * Relationships describing that all words in a sentence belong to the sentence.\n",
    "\n",
    "### Poster session 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
