{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADAM Optimization\n",
    "This paper proposes a method for stochastic gradient based optimization which quickly became very popular. It stands for Adaptive Moment estimation and has a number of nice features for an optimization algorithm.\n",
    "* Invariant to scaling of gradient.\n",
    "* Does not require a stationary objective function.\n",
    "* Performs step size reduction automatically.\n",
    "\n",
    "Objective functions are often stochastic, either from the fact that data is subsampled (minibatches) or from things like dropout. This is why the focus of this paper is on optimization of stochastic objectives with high dimensional parameter spaces.\n",
    "\n",
    "In short ADAM uses the first and second moments (mean and variance) of the gradients and combines the advantages of RMSProp and AdaGrad optimization algorithms.\n",
    "\n",
    "## Algorithm\n",
    "* Have a noisy objective function $f(\\theta)$ which is differentiable w.r.t. $\\theta$\n",
    "* Want to minimize $\\mathbb{E} \\left[ f(\\theta) \\right]$\n",
    "* Keep exponential moving average over timesteps $t$ of the gradient $m_t$ and squared gradient $\\upsilon_t$.\n",
    "    * $\\beta_1, \\beta_2 \\in [0, 1)$ controls the exponential decays respectively.\n",
    "    * These moving averages estimates of first and second order moments (mean, variance) of the gradient.\n",
    "    * The moving averages are initialized to 0's so are biased towards zero, especially in early steps and if decay rates are small ($\\beta_i \\approx 1)$.\n",
    "\n",
    "\n",
    "## Pseudocode\n",
    "<img src=\"figs/adam/adam-pseudocode.png\" width=\"60%\" height=\"60%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
