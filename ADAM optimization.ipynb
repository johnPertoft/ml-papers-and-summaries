{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADAM Optimization\n",
    "This paper proposes a method for stochastic gradient based optimization which quickly became very popular. It stands for Adaptive Moment estimation and has a number of nice features for an optimization algorithm.\n",
    "* Invariant to scaling of gradient.\n",
    "* Does not require a stationary objective function.\n",
    "* Performs step size reduction automatically.\n",
    "\n",
    "Objective functions are often stochastic, either from the fact that data is subsampled (minibatches) or from things like dropout. This is why the focus of this paper is on optimization of stochastic objectives with high dimensional parameter spaces.\n",
    "\n",
    "In short ADAM uses the first and second moments (mean and variance) of the gradients and combines the advantages of RMSProp and AdaGrad optimization algorithms.\n",
    "\n",
    "## Algorithm\n",
    "* Have a noisy objective function $f(\\theta)$ which is differentiable w.r.t. $\\theta$\n",
    "* Want to minimize $\\mathbb{E} \\left[ f(\\theta) \\right]$\n",
    "* Learning rate $\\alpha$\n",
    "* Keep exponential moving average over timesteps $t$ of the gradient $m_t$ and squared gradient $\\upsilon_t$.\n",
    "    * $\\beta_1, \\beta_2 \\in [0, 1)$ controls the exponential decays respectively.\n",
    "    * These moving averages estimates of first and second order moments (mean, variance) of the gradient.\n",
    "    * The moving averages are initialized to 0's so are biased towards zero, especially in early steps and if decay rates are small ($\\beta_i \\approx 1)$.\n",
    "    * Corrected mean and variance estimates\n",
    "        * $\\hat{m}_t = m_t\\ /\\ (1 - \\beta_1^t)$\n",
    "        * $\\hat{\\upsilon}_t = \\upsilon_t\\ /\\ (1 - \\beta_2^t)$\n",
    "* The update step is computed using the corrected gradient mean $\\hat{m}_t$ and the corrected gradient variance $\\hat{\\upsilon}_t$, (minimization) $f\\theta)$: $\\theta_t = \\theta_{t-1} - \\alpha \\cdot \\hat{m}_t\\ /\\ (\\sqrt{\\hat{\\upsilon}_t} + \\epsilon)$\n",
    "    * $\\epsilon$ for numerical stability.\n",
    "    * The update step magnitude is approximately bounded by learning rate $\\alpha$.\n",
    "    * They call $\\hat{m}_t\\ /\\ \\sqrt{\\hat{\\upsilon}_t}$ signal-to-noise ratio (SNR) and with a small SNR update steps will be closer to zero. Good because low SNR means more uncertainty of direction to go. This means that typically update steps will be small close to zero when $\\theta$ close to optimum.\n",
    "    * Invariant to scaling of gradient because $(c \\cdot \\hat{m}_t)\\ /\\ (\\sqrt{c^2 \\cdot \\hat{\\upsilon}_t}) = \\hat{m}_t\\ /\\ \\sqrt{\\hat{\\upsilon}_t}$\n",
    "\n",
    "## TODO\n",
    "sparsity in gradient per steps\n",
    "why we get zero biased estimates\n",
    "\n",
    "\n",
    "## Pseudocode\n",
    "<img src=\"figs/adam/adam-pseudocode.png\" width=\"60%\" height=\"60%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
