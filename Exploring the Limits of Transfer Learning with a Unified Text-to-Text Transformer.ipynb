{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction\n",
    "- Many recent advances in NLP can be credited to large scale pretraining which is a form of transfer learning.\n",
    "- We hope that \"general-purpose\" abilities and knowledge can be learnt in pretraining and taken advantage of in downstream tasks.\n",
    "- Most pretraining techniques in NLP is unsupervised. Lots of data available.\n",
    "\n",
    "## Problem\n",
    "- Due to the diversity of techniques to do pretraining and in downstream tasks it's difficult to compare them.\n",
    "\n",
    "## In this paper\n",
    "- Introduce a framework to allow for comparisons between the many current techniques.\n",
    "- Introduce a new very big dataset.\n",
    "- Run a bunch of experiments testing out different existing settings/techniques for pretraining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Text-to-Text Transfer Transformer (T5)\n",
    "- The basic idea is to convert all NLP problems into the same **text-to-text** format.\n",
    "- More formally, for all tasks the model will be fed a text for conditioning and tasked with generating some target text.\n",
    "- In this way, the same model, training objective and decoding technique etc can be used across all problems.\n",
    "- This framework can be used for both pre-training and fine-tuning.\n",
    "\n",
    "<img src=\"figs/t5/fig1.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Denoising pretraining objective\n",
    "- Predict missing or corrupted tokens (like in BERT).\n",
    "- The following figure describes one way of how this is converted to fit the **text-to-text** framework.\n",
    "\n",
    "<img src=\"figs/t5/fig2.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Examples from finetuning tasks\n",
    "- In general each input text is prefixed with the task name.\n",
    "- Classification tasks: The model should predict a single word that is the corresponding label.\n",
    "- Regression task: If possible (STS-B dataset) recast into classification task and then do the same. E.g. 2.57 -> \"2.6\".\n",
    "- \"Ambiguous pronoun prediction\": The ambiguous pronoun is highlighted in the input text and the output text should be the noun that is referred to.\n",
    "- See section 2.4 and appendix D for more examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Colossal Clean Crawled Corpus (C4)\n",
    "- They want to measure effects of properties of the dataset used for training so there's a need to a very diverse dataset.\n",
    "- They use the Common Crawl archive which is a publically available archive of web content.\n",
    "- The raw output of this contains a lot of junk so they apply some heuristics to clean it.\n",
    "    - Only keep lines that end in some form of terminal punctuation.\n",
    "    - Remove pages with dirty and bad words.\n",
    "    - Remove pages with placeholder text like lorem ipsum.\n",
    "    - Remove pages with curly brackets or mentions of Javascript to avoid source code.\n",
    "    - Remove duplicate pages.\n",
    "    - Remove pages that are predicted to not be English with high probability.\n",
    "- About 750GB.\n",
    "- Available via Tensorflow Datasets.\n",
    "\n",
    "**Discussion point:** Any risk of test data leaking into training data since some downstream tasks are based on data that could have been part of the scraping? They mention that they use data scraped from April 2019, but this can probably still include any Wikipedia article for example?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Experiments\n",
    "- The goal is to measure general language learning which is done by pretraining and then comparing performance on fine tuned downstream tasks.\n",
    "- As part of this the aim of the experiments is to see which recent developments have the most impact. These are\n",
    "    - Pretraining objectives.\n",
    "    - Architectures.\n",
    "    - Training strategies.\n",
    "    - Datasets.\n",
    "- Finetuning and evaluation is done on\n",
    "    - GLUE (collection of many tasks)\n",
    "    - SuperGLUE (collection of many tasks)\n",
    "    - CNN/DailyMail summarization\n",
    "    - SQuAD question answering\n",
    "    - WMT translation tasks, English to German, French, Romanian. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Experiment: Baseline\n",
    "This section describes the baseline configuration. In the following experiments unless something is explicitly changed, this is the configuration.\n",
    "\n",
    "## Model\n",
    "- Standard encoder-decoder Transformer.\n",
    "- Settings similar to BERT base (L=12) but with two stacks (encoder and decoder), ~220M params.\n",
    "- Simpler form of positional embedding than the commonly used relative position embedding (section 2.1).\n",
    "\n",
    "## Pretraining\n",
    "- Denoising objective.\n",
    "- Trained with teacher forcing and cross entropy loss.\n",
    "- Greedy decoding.\n",
    "- Sequence length 512, batch size 128. They pack multiple sequences in each batch entry as much as possible.\n",
    "- Pretrain for $2^{19}$ steps on their C4 dataset.\n",
    "- Constant learning rate (0.01) for 10000 steps and then decay it for rest of pretraining.\n",
    "- AdaFactor optimizer.\n",
    "\n",
    "## Finetuning\n",
    "- Finetune for $2^{18}$ steps on each task but report evaluation score for best checkpoint per task.\n",
    "- Same sequence and batch setup.\n",
    "- Constant learning rate (0.001)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Baseline performance\n",
    "- To get an idea of inter-run variance they train and evaluate with the baseline setup (I assume both pretrain and finetune) 10 times. They then assume this variance holds for the other experiment variants.\n",
    "- They also do finetuning only and evaluate to see the benefits of pretraining.\n",
    "\n",
    "\n",
    "<img src=\"figs/t5/table1.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Experiment: Architectures\n",
    "In this section different model architectures are considered. And also two different kinds of pretraining objectives that might be more suitable to either.\n",
    "\n",
    "## Models\n",
    "- Standard encoder-decoder Transformer of some different configurations.\n",
    "    - Same as baseline.\n",
    "    - Parameters shared between encoder and decoder.\n",
    "    - Half the number of layers in both encoder and decoder.\n",
    "- Decoder only (L=12) of some different configurations.\n",
    "    - With standard causal masking in self attention (LM).\n",
    "    - With fully visible masking on the input and then causal masking on output (prefix LM).\n",
    "    \n",
    "## Pretraining\n",
    "- Denoising objective (same as baseline).\n",
    "    - For language models the inputs and targets are concatenated.\n",
    "- Language model objective.\n",
    "    - For encoder-decoder and prefix LM a sentence is randomly split into input and target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Results\n",
    "<img src=\"figs/t5/table2.png\"/>\n",
    "\n",
    "## Takeaways\n",
    "- Sharing parameters has almost the same performance. Good way to reduce number of parameters.\n",
    "- Reducing number of layers is really bad.\n",
    "- The denoising objective is better than language modeling for pretraining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Experiment: Unsupervised Objectives\n",
    "In this section different unsupervised pretraining objectives are considered.\n",
    "- The 3 first ones are conceptually different, a language modeling task, a denoising task, and a deshuffling task.\n",
    "- The rest are variants of the denoising task.\n",
    "    - Simpler BERT style corruption.\n",
    "    - Varying corruption rate.\n",
    "    - Span corruption over token corruption.\n",
    "<img src=\"figs/t5/table3.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Results\n",
    "<img src=\"figs/t5/table4.png\" width=\"500px\"/>\n",
    "<img src=\"figs/t5/table5.png\" width=\"500px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Takeaways\n",
    "- Denoising seems to be the better pretraining task.\n",
    "- \"Replace spans\" and \"drop tokens\" both have shorter target sequences which is good because it requires less memory and is faster to train while also having the best performance here.\n",
    "- They experiment with the corruption rate and find that varying it has a limited effect and that 15% works well.\n",
    "- They experiment with different span lengths and find 3 to be slightly better than i.i.d. tokens.\n",
    "\n",
    "<img src=\"figs/t5/fig5.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Experiment: Pretraining Dataset\n",
    "In this section different pretraining datasets are considered.\n",
    "- The **C4** dataset as baseline.\n",
    "- **Unfiltered C4** dataset which is C4 without filtering steps.\n",
    "- **RealNews-like**, C4 but only from news sources.\n",
    "- **WebText-like**, C4 from longer time period and then filtered to only include pages that have been linked on Reddit.\n",
    "- **Wikipedia**\n",
    "- **Wikipedia + Toronto book corpus**, Wikipedia with the addition of books to not rely on a single domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Results\n",
    "<img src=\"figs/t5/table8.png\"/>\n",
    "\n",
    "## Takeaways\n",
    "- The filtering of C4 has a positive effect.\n",
    "- Pretraining on in domain data can be beneficial for downstream tasks.\n",
    "- They also artificially truncate C4 and see that performance decreases as the dataset size decreases. This is likely due to overfitting on the pretraining data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Experiment: Training Strategy\n",
    "In this section different methods of adapting to downstream tasks are considered. This is compared to the baseline where **all parameters** are updated during fine tuning.\n",
    "\n",
    "- **Adapter layers** in which additional dense-relu-dense blocks with inner dimensionality $d$ are added after every existing feedforward network in the transformer blocks. Then during finetuning, only these and layer normal parameters are updated. In this way fewer parameters are updated during finetuning.\n",
    "- **Gradual unfreezing** in which layers starting from close to the output of both the encoder and decoder are gradually unfrozen and finetuned. Input embeddings are updated through the whole process.\n",
    "- **Multitask learning** in which the model is trained on all tasks at once. This is instead of pretraining (I think). Evaluation is then done based on the best checkpoint for each task. As part of this they also explore different ways of mixing examples from each task.\n",
    "- **Multitask pretraining** in which they include the supervised tasks in the pretraining along with the unsupervised task and then finetune on the specific tasks. For comparison with imagenet pretraining they also attempt to pretrain on only supervised tasks. See section 3.5.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Results\n",
    "<img src=\"figs/t5/table10.png\" width=\"500px\"/>\n",
    "<img src=\"figs/t5/table11.png\" width=\"500px\"/>\n",
    "<img src=\"figs/t5/table12.png\" width=\"500px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Takeaways\n",
    "- Adapter layers work decently for low resource tasks. Need to tune the inner dimensionality.\n",
    "- Multitask learning generally underperforms pretraining and then finetuning on each separate task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Experiment: Scaling\n",
    "In this section different methods of scaling up training are considered. In the previous experiments only a small subset of the entire C4 dataset was used. They look at this by using 4x compute resources compared to the baseline which can mean\n",
    "- 4x training steps.\n",
    "- 4x model size.\n",
    "- 4x the batch size.\n",
    "- Mix of the above.\n",
    "- 4x ensemble (logits are averaged before prediction)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Results\n",
    "<img src=\"figs/t5/table13.png\" width=\"500px\"/>\n",
    "\n",
    "## Takeaways\n",
    "- Scaling up consistently improves performance.\n",
    "- Increasing model size seems better than increasing steps or batch size.\n",
    "- 4x steps and 4x batch size are similar.\n",
    "- Note that increasing the model size makes finetuning and inference take longer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Putting it all together\n",
    "\n",
    "- **Span-corruption denoising objective** instead of i.i.d. denoising objective.\n",
    "- Pretrain for **1 million steps** of **batch size 2048, sequence length 512**.\n",
    "- 5 different model sizes, up to **11B parameters** mainly scaled up through larger inner dim of the dense feed forward net.\n",
    "- **Multitask pretraining**, i.e. train on both unsupervised and supervised datasets.\n",
    "- **Beam search** instead of greedy decoding.\n",
    "- Otherwise same settings as baseline.\n",
    "- Multiple new SotA results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Results\n",
    "<img src=\"figs/t5/table14.png\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
